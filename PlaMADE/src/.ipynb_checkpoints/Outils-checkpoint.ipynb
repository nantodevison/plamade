{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK FOURRE-TOUT pour scrips PlaMADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import Connexion_Transfert as ct\n",
    "import Outils as O\n",
    "from geoalchemy2 import Geometry,WKTElement\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.geometry.point import Point\n",
    "from shapely.geometry.multipoint import MultiPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import speedups\n",
    "speedups.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees'\n",
    "#dézipper tous les dossier de Gérard\n",
    "for root,dirs, files in os.walk(dossierSrc) : \n",
    "    for f in files : \n",
    "        if f.endswith('.zip') : \n",
    "            cheminFichier=os.path.join(root, f)\n",
    "            print(cheminFichier[:-4])\n",
    "            try:\n",
    "                with zipfile.ZipFile(cheminFichier) as z:\n",
    "                    z.extractall(cheminFichier[:-4])\n",
    "                    print(f\"fichier extrait {cheminFichier[:-4]}\")\n",
    "            except:\n",
    "                print(f\"pb extraction sur dossier {cheminFichier[:-4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. creer une bdd des fichiers geostandardises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour traiter tous les fichiers d'un dossier général.suppose que la structure des tables a déjà été crée (par exemple avec l'import des des fichiers pouis truncate)\n",
    "dossierSrc=r'D:\\Boulot\\PlaMADE\\Ile-de-france\\75-Paris\\75'\n",
    "coupleFichierTable=(('N_ROUTIER_ALLURE','allure_national'),('N_ROUTIER_REVETEMENT','rvt_national'),('N_ROUTIER_ROUTE','route_national'),\n",
    "                    ('N_ROUTIER_TRAFIC','trafic_national'),('N_ROUTIER_VITESSE','vts_national'))\n",
    "#dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Auvergne-Rhone-Alpes\\Donnees_geostandardisees\\Route01_v2_dec2020'\n",
    "listEreur=[]\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    for root,dirs, files in os.walk(dossierSrc) : \n",
    "        for f in files :\n",
    "            if f.endswith(('.shp', '.dbf')) : \n",
    "                print(os.path.join(root,f))\n",
    "                if f.endswith('.shp') and 'N_ROUTIER_TRONCON' in f :\n",
    "                    try :\n",
    "                        ct.ogr2ogr_shp2pg(c.connstringOgr,os.path.join(root,f),\n",
    "                                              schema='geostandardise_src', table='troncon_national',\n",
    "                                              SRID=None,geotype='MULTILINESTRINGZ', dims=3, creationMode='-append -update',encodageClient='UTF-8', version_simple=True)\n",
    "                    except Exception as e: \n",
    "                        listEreur.append({'fichier': f, erreur : e})\n",
    "                else : \n",
    "                    for fich,t in coupleFichierTable :\n",
    "                        try : \n",
    "                            if fich in f.upper() and f.endswith('.dbf') : \n",
    "                                ct.ogr2ogr_shp2pg(c.connstringOgr,os.path.join(root,f),\n",
    "                                                  schema='geostandardise_src', table=t, SRID=None,geotype=None, dims=None, creationMode='-append -update',encodageClient='UTF-8', requeteSql='', version_simple=True)\n",
    "                            elif fich in f.upper() and f.endswith('.csv') :\n",
    "                                df = pd.read_csv(os.path.join(root,f), \n",
    "                                                 keep_default_na=False)\n",
    "                                df.columns=[c.lower() for c in df.columns]\n",
    "                                df.drop(colonnesEnTrop,axis=1).to_sql(t,c.sqlAlchemyConn,'geostandardise_src', if_exists='append', index=False )\n",
    "                        except Exception as e: \n",
    "                            listEreur.append({'fichier': f, 'erreur' : e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si les donnees trafic, vts, allure , rvt sont en csv, le plus simple c'est pandas : \n",
    "dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Bretagne'\n",
    "coupleFichierTable=(('N_ROUTIER_ALLURE','allure_national'),('N_ROUTIER_REVETEMENT','rvt_national'),('N_ROUTIER_ROUTE','route_national'),\n",
    "                    ('N_ROUTIER_TRAFIC','trafic_national'),('N_ROUTIER_VITESSE','vts_national'))\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    for root,dirs, files in os.walk(dossierSrc) : \n",
    "            for f in files :\n",
    "                if f.endswith('.csv') :\n",
    "                    for fich,t in coupleFichierTable :\n",
    "                        if fich in f.upper() : \n",
    "                            try : \n",
    "                                print(os.path.join(root,f))\n",
    "                                df = pd.read_csv(os.path.join(root,f), \n",
    "                                                keep_default_na=False)\n",
    "                                df.columns=[c.lower() for c in df.columns]\n",
    "                                dfref=pd.read_sql(f'select * from geostandardise_src.{t} limit 1',c.sqlAlchemyConn)\n",
    "                                colonnesEnTrop=[c.lower() for c in df.columns if c not in dfref.columns]\n",
    "                                df.drop(colonnesEnTrop,axis=1).to_sql(t,c.sqlAlchemyConn,'geostandardise_src', if_exists='append', index=False )\n",
    "                            except Exception as e :\n",
    "                                print(f'Erreur sur : {os.path.join(root,f)} : {e} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour test sur un dept\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    ct.ogr2ogr_shp2pg(c.connstringOgr,r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Bretagne\\35\\N_ROUTIER_TRONCON_L_035.shp',\n",
    "                                              schema='geostandardise_src', table='troncon_national',\n",
    "                                              SRID='2154',geotype='MULTILINESTRING', dims=3, creationMode='-append -update',encodageClient='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sur un fichier\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    ct.ogr2ogr_shp2pg(c.connstringOgr,r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Auvergne_Rhone_Alpes\\Donnees_geostandardisees\\Route01_v2_dec2020\\N_ROUTIER_ALLURE_001.dbf',\n",
    "                                                  schema='geostandardise_src', table='allure_national', SRID=None,geotype=None, dims=None, creationMode='-append -update',encodageClient='UTF-8', requeteSql='', version_simple=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Générer un fichier unique trafic RD\n",
    "> L'idée est de se baser sur le fichier de résumé des fichiers gestionnaires et de leurs attributs pour produire un fichier concaténé, avec des attributs uniques relatifs au :\n",
    "- tmja\n",
    "- pcpl\n",
    "- nom de la voie\n",
    "- annee du trafic\n",
    "- fichier source (sur Box internet)\n",
    "- nom de la source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Principe mise en forme des fichiers \n",
    ">> Pour faire ça on va aller chercher le [fichier de synthses des données RD] (https://cerema.app.box.com/folder/132749694470/Synthese_type_fichier.ods).\n",
    "dans l'onglet trafic, on va itérer sur chaque ligne (donc récupération du tuple de valeur) : \n",
    ">> On limite l'analyse aux données présentant un colonne valide='oui' & type='sig' & type_geom='ligne'  \n",
    "1. on récupere la valeur de la colonne \"nom_fichier_trafic\" :\n",
    "    1. si l'extenstion est présente, on va lire le fichier (attention, via box Drive, donc paramètre de raw string en entrée)\n",
    "    1. l'attribut relatif au tmja est toujours présent, celui au pc_pl parfois, comme pour les routes ou l'année, mais comme la fonction rename s'en fout, on fait le rename des 4 colonnes\n",
    "    1. si des formules sont à appliquer, on les applique (liées au pcpl, et nom de route notamment\n",
    "    1. gérer le format des dates pour l'année de mesure\n",
    "    1. ajouter les attributs sur le fichiers sources, le nom de la source, le type de source\n",
    "    1. enregistrer le fichier mise en forme dans le dossier qui va bien (paramètre entrée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouvrir le fichiers de syntheses des données\n",
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer le generateurde parcours des valeusr en tuple\n",
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction d'ajout des sources de données\n",
    "def ajouterSourceDonnees(tupleNomAttr, dfFichierRenomme, fihcierSource) : \n",
    "    \"\"\"\n",
    "    ajouter les nomsource, fichier source et type de source aux donnees mise en formes\n",
    "    in : \n",
    "        tupleNomAttr : tuple des noms d'attributs recherche dans le fchier source\n",
    "        dfFichierRenomme : gdf : contient la trsucture attributaire prevue, apres calcul\n",
    "        fihcierSource : String : nom du fichier source\n",
    "    \"\"\"\n",
    "    dfFichierRenomme['nomsource']='CD_'+tupleNomAttr.dept\n",
    "    dfFichierRenomme['fichie_src']=fihcierSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction d'ouverture et reprojection en 2154\n",
    "def ouvrirReprojeter(cheminFichier, tupleNomAttr):\n",
    "    \"\"\"\n",
    "    tupleNomAttr : tuple des noms d'attributs recherche dans le fchier source\n",
    "    \"\"\"\n",
    "    dfFichierSource=gp.read_file(cheminFichier, crs=tupleNomAttr.projection)\n",
    "    dfFichierSource=dfFichierSource.to_crs('epsg:2154')\n",
    "    return dfFichierSource\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de calcul regroupant les traitements propres a chaque dept ayant des regles de gestion\n",
    "def calculSpecifique(dfFichierSource, t, listAttrNonNull) : \n",
    "    \"\"\"\n",
    "    calculer les valeur de tmja, pcpl, nom route et année dans les cas particuklier ou des regles de gestion sont necessaire\n",
    "    in : \n",
    "        dept : string : departement sur 3 caracteres\n",
    "        dfFichierSource : gdf : gdf issue des fichiers osurces gestionnaiere\n",
    "        t : tuple des noms d'attributs recherche dans le fchier source\n",
    "        listAttrNonNull : list des attributs present dans le fichier (ayant une valuer non nulle)\n",
    "    out : \n",
    "        dfFichierRenomme : gdf : contient la trsucture attributaire prevue, apres calcul\n",
    "    \"\"\"\n",
    "    dicoRvt={'ECF':['ECF','Réparations localisées','Pavés','Bicouche','GLG bicouche','PAVES'],\n",
    "             'BBSG': ['Béton bitumineux','BBSG','Inversé','BBSG (10% recycle)','BBSG (10%)','BBSG (20%)','BBSG (30%)','BBSG (BT)','Colbifibre','COLBIFIBRE','RCS','REPRO','RSC',\n",
    "                     'Enrobés'],\n",
    "             'ES' : ['ESU','Produits Spéciaux','Monocouche DG','Monocouche MG','ESGL','ESGLg','ESGLG','ESLG','ESLG 6/10','ESLGg','ESLGl','ESLGLg','Enduit'],\n",
    "             'BBME' : ['BBME','BBME (10% recycle)'],\n",
    "             'BBM' : ['BBS', 'BBM','BBMA','BBMA phonique','BBMA PHONIQUE'],\n",
    "             'BBTM' : ['BBTM','BBHM'],\n",
    "            'BBUM': ['BBUM']}\n",
    "    def defRvt(rvt):\n",
    "        try : \n",
    "            if rvt : \n",
    "                rvtOk=[k for k, v in dicoRvt.items() if rvt in v][0]\n",
    "            else :\n",
    "                return None\n",
    "        except IndexError :\n",
    "            print(rvt)\n",
    "        return rvtOk\n",
    "    \n",
    "    \n",
    "    if t.dept=='012' : \n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TMJATV2015'] if not pd.isnull(x['TMJATV2015']) else x['TMJA_TV'], axis=1)\n",
    "        dfFichierSource['annee']=dfFichierSource.apply(lambda x : re.match('2[0-9]{3]',x['COMMENTAIR']) if not pd.isnull(x['COMMENTAIR']) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='002': \n",
    "        dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['NatureCouc']) if not pd.isnull(x['NatureCouc']) else None, axis=1)\n",
    "        dfFichierSource['anneePose']=dfFichierSource.apply(lambda x : str(x.AnneeCouch) if not x.AnneeCouc[0]=='<' else str(x.AnneeCouc)[1:], axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',t.nom_attr_trafic:'trafic',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_granulo:'granulo'})\n",
    "    elif t.dept=='008':\n",
    "        \n",
    "    elif t.dept=='025' :\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TMJA'] if x['%_PL']!='D' else None, axis=1)\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['%_PL']) if x['%_PL']!='D' else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='027' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.F_PL.apply(lambda x : x.replace('%','').replace(',','.') if not pd.isnull(x) else None)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',t.nom_attr_route:'nomRoute',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='031' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.route.apply(lambda x : O.epurationNomRoute(x))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='037' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.idroute.apply(lambda x : x.split('_')[1])\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='046': \n",
    "        dfFichierSource['trafic']=dfFichierSource['MJA TV (an'].apply(lambda x : x.replace(' ','').split('(')[0] if not pd.isnull(x) else None)\n",
    "        dfFichierSource['annee']=dfFichierSource['MJA TV (an'].apply(lambda x : x.replace(' ','').split('(')[1][:-1] if not pd.isnull(x) and '(' in x else None)\n",
    "        dfFichierSource['pcpl']=dfFichierSource['%PL'].apply(lambda x : float(x)*100 if not pd.isnull(x) else None)\n",
    "        dfFichierSource['nomRoute']=dfFichierSource['RD'].apply(lambda x : x.replace(' ','').replace('RD','D'))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='049' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.Route.apply(lambda x : O.epurationNomRoute(x[3:]))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='052' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['MJA_PL']/x['MJA_TV']*100) if not pd.isnull(x['MJA_TV']) else None, axis=1)\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.Route.apply(lambda x : O.epurationNomRoute(x))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='058' :\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TRAFFIC_VL']+x['TRAFFIC_PL'], axis=1)\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['TRAFFIC_PL']/(x['TRAFFIC_VL']+x['TRAFFIC_PL'])*100) if x['TRAFFIC_VL']+x['TRAFFIC_PL']>0 else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='059' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.VOIE.apply(lambda x : O.epurationNomRoute(x[1:]))\n",
    "        dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['TYP_DE_REV']) if not pd.isnull(x['TYP_DE_REV']) else defRvt(x['C_D_C_DE_S']), axis=1)\n",
    "        dfFichierSource['granulo']=dfFichierSource.apply(lambda x : re.match(r'([0-9]{1,2}/[0-9]{1,2})',x.GRANULOMET)[0] if not pd.isnull(x.GRANULOMET) and re.match(r'([0-9]/[0-9])',x.GRANULOMET) else None, axis=1)\n",
    "        dfFichierSource['anneePose']=dfFichierSource.apply(lambda x : str(x.D_D_C_DE_S)[:4] if not pd.isnull(x.D_D_C_DE_S) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl'})\n",
    "    elif t.dept=='060' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.PCT_PL_SEM.apply(lambda x : x.replace('%PL','') if not x=='%PL' else None)\n",
    "        dfFichierSource['annee']=dfFichierSource.DATE_COMPT.str[-4:]\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_route:'nomRoute',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='070':\n",
    "        for c in [f'TA_TV{i}' for i in range(10,19)]: \n",
    "            dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c])>0 if not pd.isnull(x[c]) else False, axis=1),'trafic']= dfFichierSource[c]\n",
    "            dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1),'pcpl']=(dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1)].apply(lambda x : float(x[c.replace('TV','PL')]), axis=1)/\n",
    "              dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1)].apply(lambda x : float(x[c]), axis=1)*100)                \n",
    "            dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c])>0 if not pd.isnull(x[c]) else False , axis=1),'annee']= f'20{c[-2:]}'\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='075' :\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : (x['MT']*12)+(x['ME']*4)+(x['MN']*8) if not (pd.isnull(x['MT']) and pd.isnull(x['ME']) and pd.isnull(x['MN'])) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept =='080' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].str[3:]\n",
    "        dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['NATURE']) if not pd.isnull(x['NATURE']) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='082' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].str[3:]\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept =='076' : \n",
    "        dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].apply(lambda x : O.epurationNomRoute(x[3:]) if x[:2]!='VC' else x )\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='077' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['PLTMJA']/x['TVTMJA']*100) if not pd.isnull(x['TVTMJA']) else None, axis=1)\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.ROUTE.apply(lambda x : O.epurationNomRoute(x[2:]))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='083' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.ROUTE.apply(lambda x : O.epurationNomRoute(x[3:]))\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : int(x['MJA_2018']) if int(x['MJA_2018'])>0 else int(x['MJA_2017']), axis=1)\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : x['PL_2018'] if int(x['MJA_2018'])>0 else str(int(x['PL_2017'])/10), axis=1)\n",
    "        dfFichierSource['annee']=dfFichierSource.apply(lambda x : '2018' if int(x['MJA_2018'])>0 else '2017', axis=1)                                                                                                  \n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    return dfFichierRenomme[listAttrNonNull]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Concat des donnees \n",
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('027') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #print(listAttrNonNull)\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('012','027', '037', '049', '052', '058', '059', '060', '075', '077','082', '083', '076','080'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcat=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'D:\\Boulot\\AffairesEnCours\\plamade\\RDs\\concat\\RD_concatenation_trafic_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.2 Principe mise en forme des fichiers ponctuels\n",
    ">> on va garder la structure précédente, mais il va aussi falloir ajouter les 2 jeu de données en tableur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('046') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('025','031', '070', '046'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "#tagguer les geoms nulles \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "dfConcat.loc[dfConcat.nomsource=='CD_008', 'annee']='2018'\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)\n",
    "\n",
    "#toutes les geoms en multi\n",
    "dfConcat[\"geometry\"] = [MultiPoint([feature]) if isinstance(feature, Point) else feature for feature in dfConcat[\"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'D:\\Boulot\\AffairesEnCours\\plamade\\RDs\\concat\\RD_concatenation_trafic_ponctuel.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Générer un fichier unique RD  \n",
    "sur la base de ce qui a été fait au dessus, on ajouter le revetement et la vitesse quand ils sont disponible au sein du même fichier source, pour les lignes puis les points.  \n",
    "ensuite il faut voir pour les fihiers multiples, on va plutot essayer de regrouper l'info au sein d'un seul fichier prétraité qui regroupera toute les infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.1 fichier lignes avec un seul fichier source et tous les attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        if t.fichier_unique=='oui' : \n",
    "            if t.dept  in  ('037','067', '068','083') : \n",
    "                    continue\n",
    "            for f in files : \n",
    "                if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                    #print(f)\n",
    "                    #ouvrir et modifier le crs si besoin\n",
    "                    dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                    dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',\n",
    "                                 t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                    listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                    #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                    if t.dept in ('012','027', '037', '049', '052', '058', '059', '060', '075', '077','082', '083', '076','080'):\n",
    "                        dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                    else :\n",
    "                        dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                    ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)\n",
    "\n",
    "#passage des NaN sur rvtType, granulo, anneePose en None\n",
    "dfConcatFixe.loc[pd.isnull(dfConcatFixe.rvtType), 'rvtType']=None\n",
    "dfConcatFixe.loc[pd.isnull(dfConcatFixe.granulo), 'granulo']=None\n",
    "dfConcatFixe.loc[pd.isnull(dfConcatFixe.anneePose), 'anneePose']=None\n",
    "\n",
    "#coller la granulo sur les codes Enumeres\n",
    "def granuloStandard(granulo) : \n",
    "    dico_granulo={'0/4' : ['0/4',],\n",
    "                    '0/6' : ['0/6',],\n",
    "                    '0/8' : ['0/8',],\n",
    "                    '0/10 type 1' : ['0/10','0/10 type 1'],\n",
    "                    '0/10 type 2' : ['0/10 type 2',],\n",
    "                    '0/14' : ['0/14',],\n",
    "                    '4/6' : ['4/6',],\n",
    "                    '6/8' : ['6/8','5/8'],\n",
    "                    '6/10' : ['6/10'],\n",
    "                    '10/14': ['10/14']}\n",
    "    if granulo : \n",
    "        granuloOk=[k for k, v in dico_granulo.items() if granulo in v][0]\n",
    "    else : \n",
    "        return None\n",
    "    return granuloOk\n",
    "dfConcatFixe['granulo']=dfConcatFixe.granulo.apply(lambda x : granuloStandard(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, '2017', '2014', '2009', '2004', '1990', '2011', '2012',\n",
       "       '2016', '2015', '2007', '2005', '2010', '2018', '2000', '2008',\n",
       "       '2002', '2006', '1997', '2003', '2019', '1984', '2001', '1998',\n",
       "       '1995', '1999', '2013', '1994', '1991', '1985', '1996', '1950',\n",
       "       '1978', '1987', '1993', '1960'], dtype=object)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple de verifs\n",
    "#dfConcatFixe.granulo.unique()\n",
    "#dfConcatFixe.anneePose.unique()\n",
    "#dfConcatFixe.rvtType.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.2 fichier point avec un seul fichier source et tous les attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('046') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',\n",
    "                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('002','008','025','031', '070', '046'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dept</th>\n",
       "      <th>nom_fichier_trafic</th>\n",
       "      <th>valide_trafic</th>\n",
       "      <th>type_trafic</th>\n",
       "      <th>type_geom_trafic</th>\n",
       "      <th>nom_attr_trafic</th>\n",
       "      <th>nom_attr_pcpl</th>\n",
       "      <th>nom_attr_route</th>\n",
       "      <th>nom_attr_annee</th>\n",
       "      <th>projection</th>\n",
       "      <th>...</th>\n",
       "      <th>fichier_unique</th>\n",
       "      <th>nom_fichier_revetement</th>\n",
       "      <th>valide_revetement</th>\n",
       "      <th>type_revetement</th>\n",
       "      <th>type_geom_revetement</th>\n",
       "      <th>nom_attr_revetement</th>\n",
       "      <th>nom_attr_granulo</th>\n",
       "      <th>nom_attr_annee_pose</th>\n",
       "      <th>regles gestion_revetement</th>\n",
       "      <th>commentaires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>geolocalisation_pt_comptage_Cerema.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>TMJA</td>\n",
       "      <td>%PL</td>\n",
       "      <td>ROUTE</td>\n",
       "      <td>ANNÉE</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>oui</td>\n",
       "      <td>geolocalisation_pt_comptage_Cerema.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>NatureCoucheRoulmt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AnneeCoucheRoulmt</td>\n",
       "      <td>oui</td>\n",
       "      <td>fihcier original en tableur : 02_COMPTAGES_SUP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>008</td>\n",
       "      <td>BZ_Compil_trafic_2018 du 08.SHP</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>TMJA_18</td>\n",
       "      <td>PCT_PL_18</td>\n",
       "      <td>ROUTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>oui</td>\n",
       "      <td>Copie de CD 08 - PPBE - TMJA sup 5 000 + Couch...</td>\n",
       "      <td>oui</td>\n",
       "      <td>tableau</td>\n",
       "      <td>ligne</td>\n",
       "      <td>NATURE_CR_LIBELLE (Couche de roulement), DETAI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DETAILS_TRAVAUX (Couche de roulement)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>025</td>\n",
       "      <td>2019_12_12_Points_Comptage_CD25.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>TMJA</td>\n",
       "      <td>%_PL</td>\n",
       "      <td>AXE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>031</td>\n",
       "      <td>CD31-Haute-Garonne.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>tmja_tra</td>\n",
       "      <td>pl_tra</td>\n",
       "      <td>route</td>\n",
       "      <td>annee_tra</td>\n",
       "      <td>Epsg:4326</td>\n",
       "      <td>...</td>\n",
       "      <td>oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>039</td>\n",
       "      <td>RO_GP_COMPT_PONCT.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>TV</td>\n",
       "      <td>TAUX_PL</td>\n",
       "      <td>AXE</td>\n",
       "      <td>ANNEE</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>046</td>\n",
       "      <td>geolocalisation_Cerema.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>MJA TV (année)</td>\n",
       "      <td>%PL</td>\n",
       "      <td>RD</td>\n",
       "      <td>attr trafic</td>\n",
       "      <td>Epsg:4326</td>\n",
       "      <td>...</td>\n",
       "      <td>oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>070</td>\n",
       "      <td>Trafic 2010 - 2018 CD 70.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>TA_TV18</td>\n",
       "      <td>POURCENT_P</td>\n",
       "      <td>ROUTE</td>\n",
       "      <td>Tous</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>089</td>\n",
       "      <td>2020-02-26__Trafic_Bruit(2018)_du_89.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>point</td>\n",
       "      <td>TMJA,N,9,0</td>\n",
       "      <td>%_PL,N,3,2</td>\n",
       "      <td>NOM_RUE,C,</td>\n",
       "      <td>DATE,D</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dept                        nom_fichier_trafic valide_trafic type_trafic  \\\n",
       "0   002    geolocalisation_pt_comptage_Cerema.shp           oui         sig   \n",
       "1   008           BZ_Compil_trafic_2018 du 08.SHP           oui         sig   \n",
       "5   025       2019_12_12_Points_Comptage_CD25.shp           oui         sig   \n",
       "7   031                    CD31-Haute-Garonne.shp           oui         sig   \n",
       "11  039                     RO_GP_COMPT_PONCT.shp           oui         sig   \n",
       "14  046                geolocalisation_Cerema.shp           oui         sig   \n",
       "25  070              Trafic 2010 - 2018 CD 70.shp           oui         sig   \n",
       "33  089  2020-02-26__Trafic_Bruit(2018)_du_89.shp           oui         sig   \n",
       "\n",
       "   type_geom_trafic nom_attr_trafic nom_attr_pcpl nom_attr_route  \\\n",
       "0             point            TMJA           %PL          ROUTE   \n",
       "1             point         TMJA_18     PCT_PL_18          ROUTE   \n",
       "5             point            TMJA          %_PL            AXE   \n",
       "7             point        tmja_tra        pl_tra          route   \n",
       "11            point              TV       TAUX_PL            AXE   \n",
       "14            point  MJA TV (année)           %PL             RD   \n",
       "25            point         TA_TV18    POURCENT_P          ROUTE   \n",
       "33            point      TMJA,N,9,0    %_PL,N,3,2     NOM_RUE,C,   \n",
       "\n",
       "   nom_attr_annee projection  ... fichier_unique  \\\n",
       "0           ANNÉE  Epsg:2154  ...            oui   \n",
       "1             NaN  Epsg:2154  ...            oui   \n",
       "5             NaN  Epsg:2154  ...            oui   \n",
       "7       annee_tra  Epsg:4326  ...            oui   \n",
       "11          ANNEE  Epsg:2154  ...            oui   \n",
       "14    attr trafic  Epsg:4326  ...            oui   \n",
       "25           Tous  Epsg:2154  ...            oui   \n",
       "33         DATE,D  Epsg:2154  ...            NaN   \n",
       "\n",
       "                               nom_fichier_revetement valide_revetement  \\\n",
       "0              geolocalisation_pt_comptage_Cerema.shp               oui   \n",
       "1   Copie de CD 08 - PPBE - TMJA sup 5 000 + Couch...               oui   \n",
       "5                                                 NaN               NaN   \n",
       "7                                                 NaN               NaN   \n",
       "11                                                NaN               NaN   \n",
       "14                                                NaN               NaN   \n",
       "25                                                NaN               NaN   \n",
       "33                                                NaN               NaN   \n",
       "\n",
       "   type_revetement type_geom_revetement  \\\n",
       "0              sig                point   \n",
       "1          tableau                ligne   \n",
       "5              NaN                  NaN   \n",
       "7              NaN                  NaN   \n",
       "11             NaN                  NaN   \n",
       "14             NaN                  NaN   \n",
       "25             NaN                  NaN   \n",
       "33             NaN                  NaN   \n",
       "\n",
       "                                  nom_attr_revetement nom_attr_granulo  \\\n",
       "0                                  NatureCoucheRoulmt              NaN   \n",
       "1   NATURE_CR_LIBELLE (Couche de roulement), DETAI...              NaN   \n",
       "5                                                 NaN              NaN   \n",
       "7                                                 NaN              NaN   \n",
       "11                                                NaN              NaN   \n",
       "14                                                NaN              NaN   \n",
       "25                                                NaN              NaN   \n",
       "33                                                NaN              NaN   \n",
       "\n",
       "                      nom_attr_annee_pose regles gestion_revetement  \\\n",
       "0                       AnneeCoucheRoulmt                       oui   \n",
       "1   DETAILS_TRAVAUX (Couche de roulement)                       NaN   \n",
       "5                                     NaN                       NaN   \n",
       "7                                     NaN                       NaN   \n",
       "11                                    NaN                       NaN   \n",
       "14                                    NaN                       NaN   \n",
       "25                                    NaN                       NaN   \n",
       "33                                    NaN                       NaN   \n",
       "\n",
       "                                         commentaires  \n",
       "0   fihcier original en tableur : 02_COMPTAGES_SUP...  \n",
       "1                                                 NaN  \n",
       "5                                                 NaN  \n",
       "7                                                 NaN  \n",
       "11                                                NaN  \n",
       "14                                                NaN  \n",
       "25                                                NaN  \n",
       "33                                                NaN  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNettoyees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
