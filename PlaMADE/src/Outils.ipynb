{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK FOURRE-TOUT pour scrips PlaMADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import Connexion_Transfert as ct\n",
    "import Outils as O\n",
    "from geoalchemy2 import Geometry,WKTElement\n",
    "from shapely.geometry import Point, LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import speedups\n",
    "speedups.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees'\n",
    "#dézipper tous les dossier de Gérard\n",
    "for root,dirs, files in os.walk(dossierSrc) : \n",
    "    for f in files : \n",
    "        if f.endswith('.zip') : \n",
    "            cheminFichier=os.path.join(root, f)\n",
    "            print(cheminFichier[:-4])\n",
    "            try:\n",
    "                with zipfile.ZipFile(cheminFichier) as z:\n",
    "                    z.extractall(cheminFichier[:-4])\n",
    "                    print(f\"fichier extrait {cheminFichier[:-4]}\")\n",
    "            except:\n",
    "                print(f\"pb extraction sur dossier {cheminFichier[:-4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. creer une bdd des fichiers geostandardises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour traiter tous les fichiers d'un dossier général.suppose que la structure des tables a déjà été crée (par exemple avec l'import des des fichiers pouis truncate)\n",
    "dossierSrc=r'D:\\Boulot\\PlaMADE\\Ile-de-france\\75-Paris\\75'\n",
    "coupleFichierTable=(('N_ROUTIER_ALLURE','allure_national'),('N_ROUTIER_REVETEMENT','rvt_national'),('N_ROUTIER_ROUTE','route_national'),\n",
    "                    ('N_ROUTIER_TRAFIC','trafic_national'),('N_ROUTIER_VITESSE','vts_national'))\n",
    "#dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Auvergne-Rhone-Alpes\\Donnees_geostandardisees\\Route01_v2_dec2020'\n",
    "listEreur=[]\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    for root,dirs, files in os.walk(dossierSrc) : \n",
    "        for f in files :\n",
    "            if f.endswith(('.shp', '.dbf')) : \n",
    "                print(os.path.join(root,f))\n",
    "                if f.endswith('.shp') and 'N_ROUTIER_TRONCON' in f :\n",
    "                    try :\n",
    "                        ct.ogr2ogr_shp2pg(c.connstringOgr,os.path.join(root,f),\n",
    "                                              schema='geostandardise_src', table='troncon_national',\n",
    "                                              SRID=None,geotype='MULTILINESTRINGZ', dims=3, creationMode='-append -update',encodageClient='UTF-8', version_simple=True)\n",
    "                    except Exception as e: \n",
    "                        listEreur.append({'fichier': f, erreur : e})\n",
    "                else : \n",
    "                    for fich,t in coupleFichierTable :\n",
    "                        try : \n",
    "                            if fich in f.upper() and f.endswith('.dbf') : \n",
    "                                ct.ogr2ogr_shp2pg(c.connstringOgr,os.path.join(root,f),\n",
    "                                                  schema='geostandardise_src', table=t, SRID=None,geotype=None, dims=None, creationMode='-append -update',encodageClient='UTF-8', requeteSql='', version_simple=True)\n",
    "                            elif fich in f.upper() and f.endswith('.csv') :\n",
    "                                df = pd.read_csv(os.path.join(root,f), \n",
    "                                                 keep_default_na=False)\n",
    "                                df.columns=[c.lower() for c in df.columns]\n",
    "                                df.drop(colonnesEnTrop,axis=1).to_sql(t,c.sqlAlchemyConn,'geostandardise_src', if_exists='append', index=False )\n",
    "                        except Exception as e: \n",
    "                            listEreur.append({'fichier': f, 'erreur' : e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si les donnees trafic, vts, allure , rvt sont en csv, le plus simple c'est pandas : \n",
    "dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Bretagne'\n",
    "coupleFichierTable=(('N_ROUTIER_ALLURE','allure_national'),('N_ROUTIER_REVETEMENT','rvt_national'),('N_ROUTIER_ROUTE','route_national'),\n",
    "                    ('N_ROUTIER_TRAFIC','trafic_national'),('N_ROUTIER_VITESSE','vts_national'))\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    for root,dirs, files in os.walk(dossierSrc) : \n",
    "            for f in files :\n",
    "                if f.endswith('.csv') :\n",
    "                    for fich,t in coupleFichierTable :\n",
    "                        if fich in f.upper() : \n",
    "                            try : \n",
    "                                print(os.path.join(root,f))\n",
    "                                df = pd.read_csv(os.path.join(root,f), \n",
    "                                                keep_default_na=False)\n",
    "                                df.columns=[c.lower() for c in df.columns]\n",
    "                                dfref=pd.read_sql(f'select * from geostandardise_src.{t} limit 1',c.sqlAlchemyConn)\n",
    "                                colonnesEnTrop=[c.lower() for c in df.columns if c not in dfref.columns]\n",
    "                                df.drop(colonnesEnTrop,axis=1).to_sql(t,c.sqlAlchemyConn,'geostandardise_src', if_exists='append', index=False )\n",
    "                            except Exception as e :\n",
    "                                print(f'Erreur sur : {os.path.join(root,f)} : {e} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour test sur un dept\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    ct.ogr2ogr_shp2pg(c.connstringOgr,r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Bretagne\\35\\N_ROUTIER_TRONCON_L_035.shp',\n",
    "                                              schema='geostandardise_src', table='troncon_national',\n",
    "                                              SRID='2154',geotype='MULTILINESTRING', dims=3, creationMode='-append -update',encodageClient='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sur un fichier\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    ct.ogr2ogr_shp2pg(c.connstringOgr,r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Auvergne_Rhone_Alpes\\Donnees_geostandardisees\\Route01_v2_dec2020\\N_ROUTIER_ALLURE_001.dbf',\n",
    "                                                  schema='geostandardise_src', table='allure_national', SRID=None,geotype=None, dims=None, creationMode='-append -update',encodageClient='UTF-8', requeteSql='', version_simple=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Générer un fichier unique trafic RD\n",
    "> L'idée est de se baser sur le fichier de résumé des fichiers gestionnaires et de leurs attributs pour produire un fichier concaténé, avec des attributs uniques relatifs au :\n",
    "- tmja\n",
    "- pcpl\n",
    "- nom de la voie\n",
    "- annee du trafic\n",
    "- fichier source (sur Box internet)\n",
    "- nom de la source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Principe mise en forme des fichiers\n",
    ">> Pour faire ça on va aller chercher le [fichier de synthses des données RD] (https://cerema.app.box.com/folder/132749694470/Synthese_type_fichier.ods).\n",
    "dans l'onglet trafic, on va itérer sur chaque ligne (donc récupération du tuple de valeur) : \n",
    ">> On limite l'analyse aux données présentant un colonne valide='oui' & type='sig' & type_geom='ligne'  \n",
    "1. on récupere la valeur de la colonne \"nom_fichier_trafic\" :\n",
    "    1. si l'extenstion est présente, on va lire le fichier (attention, via box Drive, donc paramètre de raw string en entrée)\n",
    "    1. l'attribut relatif au tmja est toujours présent, celui au pc_pl parfois, comme pour les routes ou l'année, mais comme la fonction rename s'en fout, on fait le rename des 4 colonnes\n",
    "    1. si des formules sont à appliquer, on les applique (liées au pcpl, et nom de route notamment\n",
    "    1. gérer le format des dates pour l'année de mesure\n",
    "    1. ajouter les attributs sur le fichiers sources, le nom de la source, le type de source\n",
    "    1. enregistrer le fichier mise en forme dans le dossier qui va bien (paramètre entrée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouvrir le fichiers de syntheses des données\n",
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='trafic',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer le generateurde parcours des valeusr en tuple\n",
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction d'ajout des sources de données\n",
    "def ajouterSourceDonnees(tupleNomAttr, dfFichierRenomme, fihcierSource) : \n",
    "    \"\"\"\n",
    "    ajouter les nomsource, fichier source et type de source aux donnees mise en formes\n",
    "    in : \n",
    "        tupleNomAttr : tuple des noms d'attributs recherche dans le fchier source\n",
    "        dfFichierRenomme : gdf : contient la trsucture attributaire prevue, apres calcul\n",
    "        fihcierSource : rawString : chemin du fichier source\n",
    "    \"\"\"\n",
    "    dfFichierRenomme['nomsource']='CD_'+tupleNomAttr.dept\n",
    "    dfFichierRenomme['fichiersource']=fihcierSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de calcul regroupant les traitements propres a chaque dept ayant des regles de gestion\n",
    "def calculSpecifique(dfFichierSource, tupleNomAttr, listAttrNonNull) : \n",
    "    \"\"\"\n",
    "    calculer les valeur de tmja, pcpl, nom route et année dans les cas particuklier ou des regles de gestion sont necessaire\n",
    "    in : \n",
    "        dept : string : departement sur 3 caracteres\n",
    "        dfFichierSource : gdf : gdf issue des fichiers osurces gestionnaiere\n",
    "        tupleNomAttr : tuple des noms d'attributs recherche dans le fchier source\n",
    "        listAttrNonNull : list des attributs present dans le fichier (ayant une valuer non nulle)\n",
    "    out : \n",
    "        dfFichierRenomme : gdf : contient la trsucture attributaire prevue, apres calcul\n",
    "    \"\"\"\n",
    "    if tupleNomAttr.dept=='012' : \n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TMJATV2015'] if not pd.isnull(x['TMJATV2015']) else x['TMJA_TV'], axis=1)\n",
    "        dfFichierSource['annee']=dfFichierSource.apply(lambda x : re.match('2[0-9]{3]',x['COMMENTAIR']) if not pd.isnull(x['COMMENTAIR']) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={tupleNomAttr.nom_attr_pcpl:'pcpl', tupleNomAttr.nom_attr_route:'nomRoute'})\n",
    "    elif tupleNomAttr.dept=='037' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.idroute.apply(lambda x : x.split('_')[1])\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',tupleNomAttr.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee'})\n",
    "    elif tupleNomAttr.dept=='049' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.Route.apply(lambda x : O.epurationNomRoute(x[3:]))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',tupleNomAttr.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee'})\n",
    "    elif tupleNomAttr.dept=='052' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : x['MJA_PL']/x['MJA_TV']*100 if not pd.isnull(x['MJA_TV']) else None, axis=1)\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.Route.apply(lambda x : O.epurationNomRoute(x))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee'})\n",
    "    elif tupleNomAttr.dept=='058' :\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TRAFFIC_VL']+x['TRAFFIC_PL'], axis=1)\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : x['TRAFFIC_PL']/(x['TRAFFIC_VL']+x['TRAFFIC_PL'])*100 if x['TRAFFIC_VL']+x['TRAFFIC_PL']>0 else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={tupleNomAttr.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee'})\n",
    "    elif tupleNomAttr.dept=='059' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.VOIE.apply(lambda x : O.epurationNomRoute(x[1:]))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',tupleNomAttr.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee'})\n",
    "    elif tupleNomAttr.dept=='060' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.PCT_PL_SEM.apply(lambda x : float(x.replace('%PL','')) if not x=='%PL')\n",
    "        dfFichierSource['annee']=dfFichierSource.DATE_COMPT.apply(lambda x : pd.to_datetim(x).year)\n",
    "    return dfFichierRenomme[listAttrNonNull]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat des donnees \n",
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                cheminFichierDonnees=os.path.join(root, f)\n",
    "                dfFichierSource=gp.read_file(cheminFichierDonnees)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('012', '037', '049', '052', '058', '059'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                elif t.dept in ('060', '075', '077', '082', '083') :\n",
    "                    continue\n",
    "                    #departements a traiter par la suite\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, cheminFichierDonnees)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatSansRegleGestion=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\geopandas\\geodataframe.py:852: UserWarning: Geometry column does not contain geometry.\n",
      "  warnings.warn(\"Geometry column does not contain geometry.\")\n"
     ]
    }
   ],
   "source": [
    "#export à l'arrache pas le biais d'une bdd permissive car pb export avec Fiona\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    if dfConcatSansRegleGestion.geometry.name!='geom':\n",
    "        df=O.gp_changer_nom_geom(dfConcatSansRegleGestion.loc[dfConcatSansRegleGestion.geometry.apply(lambda x : not pd.isnull(x))], 'geom')\n",
    "        \n",
    "        df.geom=df.apply(lambda x : WKTElement(x['geom'].wkt, srid=2154), axis=1)\n",
    "        df.to_sql('concat_cds',c.sqlAlchemyConn,schema='donnees_sources',if_exists='append', index=False,\n",
    "                  dtype={'geom': Geometry()} )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
