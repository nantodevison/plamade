{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK FOURRE-TOUT pour scrips PlaMADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import Connexion_Transfert as ct\n",
    "import Outils as O\n",
    "from geoalchemy2 import Geometry,WKTElement\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.geometry.point import Point\n",
    "from shapely.geometry.multipoint import MultiPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import speedups\n",
    "speedups.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees'\n",
    "#dézipper tous les dossier de Gérard\n",
    "for root,dirs, files in os.walk(dossierSrc) : \n",
    "    for f in files : \n",
    "        if f.endswith('.zip') : \n",
    "            cheminFichier=os.path.join(root, f)\n",
    "            print(cheminFichier[:-4])\n",
    "            try:\n",
    "                with zipfile.ZipFile(cheminFichier) as z:\n",
    "                    z.extractall(cheminFichier[:-4])\n",
    "                    print(f\"fichier extrait {cheminFichier[:-4]}\")\n",
    "            except:\n",
    "                print(f\"pb extraction sur dossier {cheminFichier[:-4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. creer une bdd des fichiers geostandardises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour traiter tous les fichiers d'un dossier général.suppose que la structure des tables a déjà été crée (par exemple avec l'import des des fichiers pouis truncate)\n",
    "dossierSrc=r'D:\\Boulot\\PlaMADE\\Ile-de-france\\75-Paris\\75'\n",
    "coupleFichierTable=(('N_ROUTIER_ALLURE','allure_national'),('N_ROUTIER_REVETEMENT','rvt_national'),('N_ROUTIER_ROUTE','route_national'),\n",
    "                    ('N_ROUTIER_TRAFIC','trafic_national'),('N_ROUTIER_VITESSE','vts_national'))\n",
    "#dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Auvergne-Rhone-Alpes\\Donnees_geostandardisees\\Route01_v2_dec2020'\n",
    "listEreur=[]\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    for root,dirs, files in os.walk(dossierSrc) : \n",
    "        for f in files :\n",
    "            if f.endswith(('.shp', '.dbf')) : \n",
    "                print(os.path.join(root,f))\n",
    "                if f.endswith('.shp') and 'N_ROUTIER_TRONCON' in f :\n",
    "                    try :\n",
    "                        ct.ogr2ogr_shp2pg(c.connstringOgr,os.path.join(root,f),\n",
    "                                              schema='geostandardise_src', table='troncon_national',\n",
    "                                              SRID=None,geotype='MULTILINESTRINGZ', dims=3, creationMode='-append -update',encodageClient='UTF-8', version_simple=True)\n",
    "                    except Exception as e: \n",
    "                        listEreur.append({'fichier': f, erreur : e})\n",
    "                else : \n",
    "                    for fich,t in coupleFichierTable :\n",
    "                        try : \n",
    "                            if fich in f.upper() and f.endswith('.dbf') : \n",
    "                                ct.ogr2ogr_shp2pg(c.connstringOgr,os.path.join(root,f),\n",
    "                                                  schema='geostandardise_src', table=t, SRID=None,geotype=None, dims=None, creationMode='-append -update',encodageClient='UTF-8', requeteSql='', version_simple=True)\n",
    "                            elif fich in f.upper() and f.endswith('.csv') :\n",
    "                                df = pd.read_csv(os.path.join(root,f), \n",
    "                                                 keep_default_na=False)\n",
    "                                df.columns=[c.lower() for c in df.columns]\n",
    "                                df.drop(colonnesEnTrop,axis=1).to_sql(t,c.sqlAlchemyConn,'geostandardise_src', if_exists='append', index=False )\n",
    "                        except Exception as e: \n",
    "                            listEreur.append({'fichier': f, 'erreur' : e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si les donnees trafic, vts, allure , rvt sont en csv, le plus simple c'est pandas : \n",
    "dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Bretagne'\n",
    "coupleFichierTable=(('N_ROUTIER_ALLURE','allure_national'),('N_ROUTIER_REVETEMENT','rvt_national'),('N_ROUTIER_ROUTE','route_national'),\n",
    "                    ('N_ROUTIER_TRAFIC','trafic_national'),('N_ROUTIER_VITESSE','vts_national'))\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    for root,dirs, files in os.walk(dossierSrc) : \n",
    "            for f in files :\n",
    "                if f.endswith('.csv') :\n",
    "                    for fich,t in coupleFichierTable :\n",
    "                        if fich in f.upper() : \n",
    "                            try : \n",
    "                                print(os.path.join(root,f))\n",
    "                                df = pd.read_csv(os.path.join(root,f), \n",
    "                                                keep_default_na=False)\n",
    "                                df.columns=[c.lower() for c in df.columns]\n",
    "                                dfref=pd.read_sql(f'select * from geostandardise_src.{t} limit 1',c.sqlAlchemyConn)\n",
    "                                colonnesEnTrop=[c.lower() for c in df.columns if c not in dfref.columns]\n",
    "                                df.drop(colonnesEnTrop,axis=1).to_sql(t,c.sqlAlchemyConn,'geostandardise_src', if_exists='append', index=False )\n",
    "                            except Exception as e :\n",
    "                                print(f'Erreur sur : {os.path.join(root,f)} : {e} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour test sur un dept\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    ct.ogr2ogr_shp2pg(c.connstringOgr,r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Bretagne\\35\\N_ROUTIER_TRONCON_L_035.shp',\n",
    "                                              schema='geostandardise_src', table='troncon_national',\n",
    "                                              SRID='2154',geotype='MULTILINESTRING', dims=3, creationMode='-append -update',encodageClient='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sur un fichier\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    ct.ogr2ogr_shp2pg(c.connstringOgr,r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Auvergne_Rhone_Alpes\\Donnees_geostandardisees\\Route01_v2_dec2020\\N_ROUTIER_ALLURE_001.dbf',\n",
    "                                                  schema='geostandardise_src', table='allure_national', SRID=None,geotype=None, dims=None, creationMode='-append -update',encodageClient='UTF-8', requeteSql='', version_simple=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Générer un fichier unique trafic RD\n",
    "> L'idée est de se baser sur le fichier de résumé des fichiers gestionnaires et de leurs attributs pour produire un fichier concaténé, avec des attributs uniques relatifs au :\n",
    "- tmja\n",
    "- pcpl\n",
    "- nom de la voie\n",
    "- annee du trafic\n",
    "- fichier source (sur Box internet)\n",
    "- nom de la source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Principe mise en forme des fichiers \n",
    ">> Pour faire ça on va aller chercher le [fichier de synthses des données RD] (https://cerema.app.box.com/folder/132749694470/Synthese_type_fichier.ods).\n",
    "dans l'onglet trafic, on va itérer sur chaque ligne (donc récupération du tuple de valeur) : \n",
    ">> On limite l'analyse aux données présentant un colonne valide='oui' & type='sig' & type_geom='ligne'  \n",
    "1. on récupere la valeur de la colonne \"nom_fichier_trafic\" :\n",
    "    1. si l'extenstion est présente, on va lire le fichier (attention, via box Drive, donc paramètre de raw string en entrée)\n",
    "    1. l'attribut relatif au tmja est toujours présent, celui au pc_pl parfois, comme pour les routes ou l'année, mais comme la fonction rename s'en fout, on fait le rename des 4 colonnes\n",
    "    1. si des formules sont à appliquer, on les applique (liées au pcpl, et nom de route notamment\n",
    "    1. gérer le format des dates pour l'année de mesure\n",
    "    1. ajouter les attributs sur le fichiers sources, le nom de la source, le type de source\n",
    "    1. enregistrer le fichier mise en forme dans le dossier qui va bien (paramètre entrée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouvrir le fichiers de syntheses des données\n",
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer le generateurde parcours des valeusr en tuple\n",
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction d'ajout des sources de données\n",
    "def ajouterSourceDonnees(tupleNomAttr, dfFichierRenomme, fihcierSource) : \n",
    "    \"\"\"\n",
    "    ajouter les nomsource, fichier source et type de source aux donnees mise en formes\n",
    "    in : \n",
    "        tupleNomAttr : tuple des noms d'attributs recherche dans le fchier source\n",
    "        dfFichierRenomme : gdf : contient la trsucture attributaire prevue, apres calcul\n",
    "        fihcierSource : String : nom du fichier source\n",
    "    \"\"\"\n",
    "    dfFichierRenomme['nomsource']='CD_'+tupleNomAttr.dept\n",
    "    dfFichierRenomme['fichie_src']=fihcierSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction d'ouverture et reprojection en 2154\n",
    "def ouvrirReprojeter(cheminFichier, tupleNomAttr):\n",
    "    \"\"\"\n",
    "    tupleNomAttr : tuple des noms d'attributs recherche dans le fchier source\n",
    "    \"\"\"\n",
    "    dfFichierSource=gp.read_file(cheminFichier, crs=tupleNomAttr.projection)\n",
    "    dfFichierSource=dfFichierSource.to_crs('epsg:2154')\n",
    "    return dfFichierSource\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de calcul regroupant les traitements propres a chaque dept ayant des regles de gestion\n",
    "def calculSpecifique(dfFichierSource, t, listAttrNonNull) : \n",
    "    \"\"\"\n",
    "    calculer les valeur de tmja, pcpl, nom route et année dans les cas particuklier ou des regles de gestion sont necessaire\n",
    "    in : \n",
    "        dept : string : departement sur 3 caracteres\n",
    "        dfFichierSource : gdf : gdf issue des fichiers osurces gestionnaiere\n",
    "        t : tuple des noms d'attributs recherche dans le fchier source\n",
    "        listAttrNonNull : list des attributs present dans le fichier (ayant une valuer non nulle)\n",
    "    out : \n",
    "        dfFichierRenomme : gdf : contient la trsucture attributaire prevue, apres calcul\n",
    "    \"\"\"\n",
    "    dicoRvt={'ECF':['ECF','Réparations localisées','Pavés','Bicouche','GLG bicouche','PAVES'],\n",
    "             'BBSG': ['Béton bitumineux','BBSG','Inversé','BBSG (10% recycle)','BBSG (10%)','BBSG (20%)','BBSG (30%)','BBSG (BT)','Colbifibre','COLBIFIBRE','RCS','REPRO','RSC',\n",
    "                     'Enrobés'],\n",
    "             'ES' : ['ESU','Produits Spéciaux','Monocouche DG','Monocouche MG','ESGL','ESGLg','ESGLG','ESLG','ESLG 6/10','ESLGg','ESLGl','ESLGLg','Enduit'],\n",
    "             'BBME' : ['BBME','BBME (10% recycle)'],\n",
    "             'BBM' : ['BBS', 'BBM','BBMA','BBMA phonique','BBMA PHONIQUE'],\n",
    "             'BBTM' : ['BBTM','BBHM'],\n",
    "            'BBUM': ['BBUM']}\n",
    "    def defRvt(rvt):\n",
    "        try : \n",
    "            if rvt : \n",
    "                rvtOk=[k for k, v in dicoRvt.items() if rvt in v][0]\n",
    "            else :\n",
    "                return None\n",
    "        except IndexError :\n",
    "            print(rvt)\n",
    "        return rvtOk\n",
    "    \n",
    "    \n",
    "    if t.dept=='012' : \n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TMJATV2015'] if not pd.isnull(x['TMJATV2015']) else x['TMJA_TV'], axis=1)\n",
    "        dfFichierSource['annee']=dfFichierSource.apply(lambda x : re.match('2[0-9]{3]',x['COMMENTAIR']) if not pd.isnull(x['COMMENTAIR']) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='002': \n",
    "        dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['NatureCouc']) if not pd.isnull(x['NatureCouc']) else None, axis=1)\n",
    "        dfFichierSource['anneePose']=dfFichierSource.apply(lambda x : str(x.AnneeCouch) if not x.AnneeCouch[0]=='<' else str(x.AnneeCouch)[1:], axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',t.nom_attr_trafic:'trafic',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_granulo:'granulo'})\n",
    "    elif t.dept=='025' :\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TMJA'] if x['%_PL']!='D' else None, axis=1)\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['%_PL']) if x['%_PL']!='D' else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='027' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.F_PL.apply(lambda x : x.replace('%','').replace(',','.') if not pd.isnull(x) else None)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',t.nom_attr_route:'nomRoute',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='031' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.route.apply(lambda x : O.epurationNomRoute(x))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='037' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.idroute.apply(lambda x : x.split('_')[1])\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='046': \n",
    "        dfFichierSource['trafic']=dfFichierSource['MJA TV (an'].apply(lambda x : x.replace(' ','').split('(')[0] if not pd.isnull(x) else None)\n",
    "        dfFichierSource['annee']=dfFichierSource['MJA TV (an'].apply(lambda x : x.replace(' ','').split('(')[1][:-1] if not pd.isnull(x) and '(' in x else None)\n",
    "        dfFichierSource['pcpl']=dfFichierSource['%PL'].apply(lambda x : float(x)*100 if not pd.isnull(x) else None)\n",
    "        dfFichierSource['nomRoute']=dfFichierSource['RD'].apply(lambda x : x.replace(' ','').replace('RD','D'))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='049' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.Route.apply(lambda x : O.epurationNomRoute(x[3:]))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='052' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['MJA_PL']/x['MJA_TV']*100) if not pd.isnull(x['MJA_TV']) else None, axis=1)\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.Route.apply(lambda x : O.epurationNomRoute(x))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='058' :\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TRAFFIC_VL']+x['TRAFFIC_PL'], axis=1)\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['TRAFFIC_PL']/(x['TRAFFIC_VL']+x['TRAFFIC_PL'])*100) if x['TRAFFIC_VL']+x['TRAFFIC_PL']>0 else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='059' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.VOIE.apply(lambda x : O.epurationNomRoute(x[1:]))\n",
    "        dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['TYP_DE_REV']) if not pd.isnull(x['TYP_DE_REV']) else defRvt(x['C_D_C_DE_S']), axis=1)\n",
    "        dfFichierSource['granulo']=dfFichierSource.apply(lambda x : re.match(r'([0-9]{1,2}/[0-9]{1,2})',x.GRANULOMET)[0] if not pd.isnull(x.GRANULOMET) and re.match(r'([0-9]/[0-9])',x.GRANULOMET) else None, axis=1)\n",
    "        dfFichierSource['anneePose']=dfFichierSource.apply(lambda x : str(x.D_D_C_DE_S)[:4] if not pd.isnull(x.D_D_C_DE_S) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl'})\n",
    "    elif t.dept=='060' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.PCT_PL_SEM.apply(lambda x : x.replace('%PL','') if not x=='%PL' else None)\n",
    "        dfFichierSource['annee']=dfFichierSource.DATE_COMPT.str[-4:]\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_route:'nomRoute',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='070':\n",
    "        for c in [f'TA_TV{i}' for i in range(10,19)]: \n",
    "            dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c])>0 if not pd.isnull(x[c]) else False, axis=1),'trafic']= dfFichierSource[c]\n",
    "            dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1),'pcpl']=(dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1)].apply(lambda x : float(x[c.replace('TV','PL')]), axis=1)/\n",
    "              dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1)].apply(lambda x : float(x[c]), axis=1)*100)                \n",
    "            dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c])>0 if not pd.isnull(x[c]) else False , axis=1),'annee']= f'20{c[-2:]}'\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='075' :\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : (x['MT']*12)+(x['ME']*4)+(x['MN']*8) if not (pd.isnull(x['MT']) and pd.isnull(x['ME']) and pd.isnull(x['MN'])) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept =='080' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].str[3:]\n",
    "        dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['NATURE']) if not pd.isnull(x['NATURE']) else None, axis=1)\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='082' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].str[3:]\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept =='076' : \n",
    "        dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].apply(lambda x : O.epurationNomRoute(x[3:]) if x[:2]!='VC' else x )\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='077' :\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['PLTMJA']/x['TVTMJA']*100) if not pd.isnull(x['TVTMJA']) else None, axis=1)\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.ROUTE.apply(lambda x : O.epurationNomRoute(x[2:]))\n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',\n",
    "                                                        t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    elif t.dept=='083' :\n",
    "        dfFichierSource['nomRoute']=dfFichierSource.ROUTE.apply(lambda x : O.epurationNomRoute(x[3:]))\n",
    "        dfFichierSource['trafic']=dfFichierSource.apply(lambda x : int(x['MJA_2018']) if int(x['MJA_2018'])>0 else int(x['MJA_2017']), axis=1)\n",
    "        dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : x['PL_2018'] if int(x['MJA_2018'])>0 else str(int(x['PL_2017'])/10), axis=1)\n",
    "        dfFichierSource['annee']=dfFichierSource.apply(lambda x : '2018' if int(x['MJA_2018'])>0 else '2017', axis=1)                                                                                                  \n",
    "        dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    return dfFichierRenomme[listAttrNonNull]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Concat des donnees \n",
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('027') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #print(listAttrNonNull)\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('012','027', '037', '049', '052', '058', '059', '060', '075', '077','082', '083', '076','080'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcat=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'D:\\Boulot\\AffairesEnCours\\plamade\\RDs\\concat\\RD_concatenation_trafic_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.2 Principe mise en forme des fichiers ponctuels\n",
    ">> on va garder la structure précédente, mais il va aussi falloir ajouter les 2 jeu de données en tableur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('046') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('025','031', '070', '046'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "#tagguer les geoms nulles \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "dfConcat.loc[dfConcat.nomsource=='CD_008', 'annee']='2018'\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)\n",
    "\n",
    "#toutes les geoms en multi\n",
    "dfConcat[\"geometry\"] = [MultiPoint([feature]) if isinstance(feature, Point) else feature for feature in dfConcat[\"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'D:\\Boulot\\AffairesEnCours\\plamade\\RDs\\concat\\RD_concatenation_trafic_ponctuel.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Générer un fichier unique RD  \n",
    "sur la base de ce qui a été fait au dessus, on ajouter le revetement et la vitesse quand ils sont disponible au sein du même fichier source, pour les lignes puis les points.  \n",
    "ensuite il faut voir pour les fihiers multiples, on va plutot essayer de regrouper l'info au sein d'un seul fichier prétraité qui regroupera toute les infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.1 fichier lignes avec un seul fichier source et tous les attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        if t.fichier_unique=='oui' : \n",
    "            if t.dept  in  ('037','067', '068','083') : \n",
    "                    continue\n",
    "            for f in files : \n",
    "                if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                    #print(f)\n",
    "                    #ouvrir et modifier le crs si besoin\n",
    "                    dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                    dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',\n",
    "                                 t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                    listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                    #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                    if t.dept in ('012','027', '037', '049', '052', '058', '059', '060', '075', '077','082', '083', '076','080'):\n",
    "                        dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                    else :\n",
    "                        dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                    ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)\n",
    "\n",
    "#passage des NaN sur rvtType, granulo, anneePose en None\n",
    "dfConcatFixe.loc[pd.isnull(dfConcatFixe.rvtType), 'rvtType']=None\n",
    "dfConcatFixe.loc[pd.isnull(dfConcatFixe.granulo), 'granulo']=None\n",
    "dfConcatFixe.loc[pd.isnull(dfConcatFixe.anneePose), 'anneePose']=None\n",
    "\n",
    "#coller la granulo sur les codes Enumeres\n",
    "def granuloStandard(granulo) : \n",
    "    dico_granulo={'0/4' : ['0/4',],\n",
    "                    '0/6' : ['0/6',],\n",
    "                    '0/8' : ['0/8',],\n",
    "                    '0/10 type 1' : ['0/10','0/10 type 1'],\n",
    "                    '0/10 type 2' : ['0/10 type 2',],\n",
    "                    '0/14' : ['0/14',],\n",
    "                    '4/6' : ['4/6',],\n",
    "                    '6/8' : ['6/8','5/8'],\n",
    "                    '6/10' : ['6/10'],\n",
    "                    '10/14': ['10/14']}\n",
    "    if granulo : \n",
    "        granuloOk=[k for k, v in dico_granulo.items() if granulo in v][0]\n",
    "    else : \n",
    "        return None\n",
    "    return granuloOk\n",
    "dfConcatFixe['granulo']=dfConcatFixe.granulo.apply(lambda x : granuloStandard(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, '2017', '2014', '2009', '2004', '1990', '2011', '2012',\n",
       "       '2016', '2015', '2007', '2005', '2010', '2018', '2000', '2008',\n",
       "       '2002', '2006', '1997', '2003', '2019', '1984', '2001', '1998',\n",
       "       '1995', '1999', '2013', '1994', '1991', '1985', '1996', '1950',\n",
       "       '1978', '1987', '1993', '1960'], dtype=object)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple de verifs\n",
    "#dfConcatFixe.granulo.unique()\n",
    "#dfConcatFixe.anneePose.unique()\n",
    "#dfConcatFixe.rvtType.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.2 fichier point avec un seul fichier source et tous les attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('046') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',\n",
    "                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('002','025','031', '070', '046'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)\n",
    "\n",
    "#passage des NaN sur rvtType, granulo, anneePose en None\n",
    "dfConcatFixe.loc[pd.isnull(dfConcatFixe.rvtType), 'rvtType']=None\n",
    "#dfConcatFixe.loc[pd.isnull(dfConcatFixe.granulo), 'granulo']=None\n",
    "dfConcatFixe.loc[pd.isnull(dfConcatFixe.anneePose), 'anneePose']=None\n",
    "\n",
    "#coller la granulo sur les codes Enumeres\n",
    "def granuloStandard(granulo) : \n",
    "    dico_granulo={'0/4' : ['0/4',],\n",
    "                    '0/6' : ['0/6',],\n",
    "                    '0/8' : ['0/8',],\n",
    "                    '0/10 type 1' : ['0/10','0/10 type 1'],\n",
    "                    '0/10 type 2' : ['0/10 type 2',],\n",
    "                    '0/14' : ['0/14',],\n",
    "                    '4/6' : ['4/6',],\n",
    "                    '6/8' : ['6/8','5/8'],\n",
    "                    '6/10' : ['6/10'],\n",
    "                    '10/14': ['10/14']}\n",
    "    if granulo : \n",
    "        granuloOk=[k for k, v in dico_granulo.items() if granulo in v][0]\n",
    "    else : \n",
    "        return None\n",
    "    return granuloOk\n",
    "#dfConcatFixe['granulo']=dfConcatFixe.granulo.apply(lambda x : granuloStandard(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BBSG', 'ES', None], dtype=object)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple de verifs\n",
    "#dfConcatFixe.granulo.unique()\n",
    "#dfConcatFixe.anneePose.unique()\n",
    "dfConcatFixe.rvtType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
