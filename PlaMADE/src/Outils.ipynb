{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK FOURRE-TOUT pour scrips PlaMADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "import os, re, shutil\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import Connexion_Transfert as ct\n",
    "import Outils as O\n",
    "from geoalchemy2 import Geometry,WKTElement\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.geometry.point import Point\n",
    "from shapely.geometry.multipoint import MultiPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import speedups\n",
    "speedups.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees'\n",
    "#dézipper tous les dossier de Gérard\n",
    "for root,dirs, files in os.walk(dossierSrc) : \n",
    "    for f in files : \n",
    "        if f.endswith('.zip') : \n",
    "            cheminFichier=os.path.join(root, f)\n",
    "            print(cheminFichier[:-4])\n",
    "            try:\n",
    "                with zipfile.ZipFile(cheminFichier) as z:\n",
    "                    z.extractall(cheminFichier[:-4])\n",
    "                    print(f\"fichier extrait {cheminFichier[:-4]}\")\n",
    "            except:\n",
    "                print(f\"pb extraction sur dossier {cheminFichier[:-4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. creer une bdd des fichiers geostandardises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour traiter tous les fichiers d'un dossier général.suppose que la structure des tables a déjà été crée (par exemple avec l'import des des fichiers pouis truncate)\n",
    "dossierSrc=r'D:\\Boulot\\PlaMADE\\Ile-de-france\\75-Paris\\75'\n",
    "coupleFichierTable=(('N_ROUTIER_ALLURE','allure_national'),('N_ROUTIER_REVETEMENT','rvt_national'),('N_ROUTIER_ROUTE','route_national'),\n",
    "                    ('N_ROUTIER_TRAFIC','trafic_national'),('N_ROUTIER_VITESSE','vts_national'))\n",
    "#dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Auvergne-Rhone-Alpes\\Donnees_geostandardisees\\Route01_v2_dec2020'\n",
    "listEreur=[]\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    for root,dirs, files in os.walk(dossierSrc) : \n",
    "        for f in files :\n",
    "            if f.endswith(('.shp', '.dbf')) : \n",
    "                print(os.path.join(root,f))\n",
    "                if f.endswith('.shp') and 'N_ROUTIER_TRONCON' in f :\n",
    "                    try :\n",
    "                        ct.ogr2ogr_shp2pg(c.connstringOgr,os.path.join(root,f),\n",
    "                                              schema='geostandardise_src', table='troncon_national',\n",
    "                                              SRID=None,geotype='MULTILINESTRINGZ', dims=3, creationMode='-append -update',encodageClient='UTF-8', version_simple=True)\n",
    "                    except Exception as e: \n",
    "                        listEreur.append({'fichier': f, erreur : e})\n",
    "                else : \n",
    "                    for fich,t in coupleFichierTable :\n",
    "                        try : \n",
    "                            if fich in f.upper() and f.endswith('.dbf') : \n",
    "                                ct.ogr2ogr_shp2pg(c.connstringOgr,os.path.join(root,f),\n",
    "                                                  schema='geostandardise_src', table=t, SRID=None,geotype=None, dims=None, creationMode='-append -update',encodageClient='UTF-8', requeteSql='', version_simple=True)\n",
    "                            elif fich in f.upper() and f.endswith('.csv') :\n",
    "                                df = pd.read_csv(os.path.join(root,f), \n",
    "                                                 keep_default_na=False)\n",
    "                                df.columns=[c.lower() for c in df.columns]\n",
    "                                df.drop(colonnesEnTrop,axis=1).to_sql(t,c.sqlAlchemyConn,'geostandardise_src', if_exists='append', index=False )\n",
    "                        except Exception as e: \n",
    "                            listEreur.append({'fichier': f, 'erreur' : e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si les donnees trafic, vts, allure , rvt sont en csv, le plus simple c'est pandas : \n",
    "dossierSrc=r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Bretagne'\n",
    "coupleFichierTable=(('N_ROUTIER_ALLURE','allure_national'),('N_ROUTIER_REVETEMENT','rvt_national'),('N_ROUTIER_ROUTE','route_national'),\n",
    "                    ('N_ROUTIER_TRAFIC','trafic_national'),('N_ROUTIER_VITESSE','vts_national'))\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    for root,dirs, files in os.walk(dossierSrc) : \n",
    "            for f in files :\n",
    "                if f.endswith('.csv') :\n",
    "                    for fich,t in coupleFichierTable :\n",
    "                        if fich in f.upper() : \n",
    "                            try : \n",
    "                                print(os.path.join(root,f))\n",
    "                                df = pd.read_csv(os.path.join(root,f), \n",
    "                                                keep_default_na=False)\n",
    "                                df.columns=[c.lower() for c in df.columns]\n",
    "                                dfref=pd.read_sql(f'select * from geostandardise_src.{t} limit 1',c.sqlAlchemyConn)\n",
    "                                colonnesEnTrop=[c.lower() for c in df.columns if c not in dfref.columns]\n",
    "                                df.drop(colonnesEnTrop,axis=1).to_sql(t,c.sqlAlchemyConn,'geostandardise_src', if_exists='append', index=False )\n",
    "                            except Exception as e :\n",
    "                                print(f'Erreur sur : {os.path.join(root,f)} : {e} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour test sur un dept\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    ct.ogr2ogr_shp2pg(c.connstringOgr,r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Bretagne\\35\\N_ROUTIER_TRONCON_L_035.shp',\n",
    "                                              schema='geostandardise_src', table='troncon_national',\n",
    "                                              SRID='2154',geotype='MULTILINESTRING', dims=3, creationMode='-append -update',encodageClient='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sur un fichier\n",
    "with ct.ConnexionBdd('local_PlaMADE', 'maison') as c :\n",
    "    ct.ogr2ogr_shp2pg(c.connstringOgr,r'C:\\Users\\martin.schoreisz\\Box\\Donnees_source\\Donnees_geostandardisees\\Auvergne_Rhone_Alpes\\Donnees_geostandardisees\\Route01_v2_dec2020\\N_ROUTIER_ALLURE_001.dbf',\n",
    "                                                  schema='geostandardise_src', table='allure_national', SRID=None,geotype=None, dims=None, creationMode='-append -update',encodageClient='UTF-8', requeteSql='', version_simple=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Générer un fichier unique trafic RD\n",
    "> L'idée est de se baser sur le fichier de résumé des fichiers gestionnaires et de leurs attributs pour produire un fichier concaténé, avec des attributs uniques relatifs au :\n",
    "- tmja\n",
    "- pcpl\n",
    "- nom de la voie\n",
    "- annee du trafic\n",
    "- fichier source (sur Box internet)\n",
    "- nom de la source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Principe mise en forme des fichiers \n",
    ">> Pour faire ça on va aller chercher le [fichier de synthses des données RD] (https://cerema.app.box.com/folder/132749694470/Synthese_type_fichier.ods).\n",
    "dans l'onglet trafic, on va itérer sur chaque ligne (donc récupération du tuple de valeur) : \n",
    ">> On limite l'analyse aux données présentant un colonne valide='oui' & type='sig' & type_geom='ligne'  \n",
    "1. on récupere la valeur de la colonne \"nom_fichier_trafic\" :\n",
    "    1. si l'extenstion est présente, on va lire le fichier (attention, via box Drive, donc paramètre de raw string en entrée)\n",
    "    1. l'attribut relatif au tmja est toujours présent, celui au pc_pl parfois, comme pour les routes ou l'année, mais comme la fonction rename s'en fout, on fait le rename des 4 colonnes\n",
    "    1. si des formules sont à appliquer, on les applique (liées au pcpl, et nom de route notamment\n",
    "    1. gérer le format des dates pour l'année de mesure\n",
    "    1. ajouter les attributs sur le fichiers sources, le nom de la source, le type de source\n",
    "    1. enregistrer le fichier mise en forme dans le dossier qui va bien (paramètre entrée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouvrir le fichiers de syntheses des données\n",
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer le generateurde parcours des valeusr en tuple\n",
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='ligne')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction d'ajout des sources de données\n",
    "def ajouterSourceDonnees(tupleNomAttr, dfFichierRenomme, fihcierSource) : \n",
    "    \"\"\"\n",
    "    ajouter les nomsource, fichier source et type de source aux donnees mise en formes\n",
    "    in : \n",
    "        tupleNomAttr : tuple des noms d'attributs recherche dans le fchier source\n",
    "        dfFichierRenomme : gdf : contient la trsucture attributaire prevue, apres calcul\n",
    "        fihcierSource : String : nom du fichier source\n",
    "    \"\"\"\n",
    "    dfFichierRenomme['nomsource']='Commune_'+tupleNomAttr.Agglo\n",
    "    dfFichierRenomme['fichie_src']=fihcierSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction d'ouverture et reprojection en 2154\n",
    "def ouvrirReprojeter(cheminFichier, tupleNomAttr):\n",
    "    \"\"\"\n",
    "    tupleNomAttr : tuple des noms d'attributs recherche dans le fchier source\n",
    "    \"\"\"\n",
    "    dfFichierSource=gp.read_file(cheminFichier, crs=tupleNomAttr.projection)\n",
    "    if not dfFichierSource.crs : \n",
    "        dfFichierSource=dfFichierSource.set_crs(tupleNomAttr.projection)\n",
    "    dfFichierSource=dfFichierSource.to_crs('epsg:2154')\n",
    "    return dfFichierSource\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de calcul regroupant les traitements propres a chaque dept ayant des regles de gestion\n",
    "\n",
    "def calculSpecifique(dfFichierSource, t, listAttrNonNull, typeAtr='trafic', typegest='dept') : \n",
    "    \"\"\"\n",
    "    calculer les valeur de tmja, pcpl, nom route et année dans les cas particuklier ou des regles de gestion sont necessaire\n",
    "    in : \n",
    "        dfFichierSource : gdf : gdf issue des fichiers osurces gestionnaiere\n",
    "        t : tuple des noms d'attributs recherche dans le fchier source\n",
    "        listAttrNonNull : list des attributs present dans le fichier (ayant une valuer non nulle)\n",
    "        typeAtr : str : 'trafic' ou 'vts' ou'rvt' defaukt 'trafic' : traduit le type de thématique du fichier ouvert\n",
    "    out : \n",
    "        dfFichierRenomme : gdf : contient la trsucture attributaire prevue, apres calcul\n",
    "    \"\"\"\n",
    "    dicoRvt={'ECF':['ECF','Réparations localisées','Pavés','Bicouche','GLG bicouche','PAVES', 'PAVE','Enrobé Coulé à Froid','ENROBE COULE A FROID','ECF cl. A Bicouche 0/10','Pavés','ENROBES COULES A FROID',\n",
    "                   'ECF cl. A Bicouche 0/6,3','Bicouche','Pavés'],\n",
    "             'BBSG': ['Béton bitumineux','BBSG','Inversé','BBSG (10% recycle)','BBSG (10%)','BBSG (20%)','BBSG (30%)','BBSG (BT)','Colbifibre','COLBIFIBRE','RCS','REPRO','RSC',\n",
    "                     'Enrobés','AUTRE','EB10','Béton Bitumineux Autre','Béton Bitumineux Froid','EB10 Tiède','Revêtement Haute Adhérence','Non connu','Non Revêtu',\n",
    "                     'PATA','CALIBRAGE','ENROBE','REPROFILAGE','?','-','BB','BBF','BBS','BBSG','EME','GB','TN18','X','BB','BBF', 'ENROBE SEMI-GRENU', 'ENROBES', 'AUTRE'],\n",
    "             'ES' : ['ESU','Produits Spéciaux','Monocouche DG','Monocouche MG','ESGL','ESGLg','ESGLG','ESLG','ESLG 6/10','ESLGg','ESLGl','ESLGLg','Enduit','Enduit Superficiel',\n",
    "                    'ENDUIT SUPERFICIEL','ESEPAIS','AC','ES','ESBIC','ESGLG','ESMDG','ESMONO', 'ESU', 'ENDUITS SUPERFICIELS', 'REPROFILAGE'],\n",
    "             'BBME' : ['BBME','BBME (10% recycle)', 'ENROBE MINCE A MODULE ELEVE',],\n",
    "             'BBM' : ['BBS', 'BBM','BBMA','BBMA phonique','BBMA PHONIQUE', 'BBMPHO','ENROBE MINCE','BBMa', 'BBMa '],\n",
    "             'BBTM' : ['BBTM','BBHM','Béton Bitumineux Très Mince','BB Très Mince Acoustique', 'BBTMPHO','ENROBE TRES MINCE','Inconnu','Autre','Asphalte','Enrobé','Herbeux','Mixte'],\n",
    "            'BBUM': ['BBUM','ULTRA MINCE','ENROBE ULTRA MINCE'],\n",
    "            'BBDr':['BBDR',],\n",
    "            'BC':['BC','BBC','Béton','Dallage']}\n",
    "    \n",
    "    def defRvt(rvt):\n",
    "        if rvt :\n",
    "            try : \n",
    "                rvtOk=[k for k, v in dicoRvt.items() if rvt in v][0]\n",
    "            except IndexError : \n",
    "                print(f'ATTENTION {rvt}')\n",
    "                return None\n",
    "        else :\n",
    "            return None\n",
    "        return rvtOk\n",
    "    \n",
    "    if typegest=='dept': \n",
    "        if t.dept=='012' : \n",
    "            dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TMJATV2015'] if not pd.isnull(x['TMJATV2015']) else x['TMJA_TV'], axis=1)\n",
    "            dfFichierSource['annee']=dfFichierSource.apply(lambda x : re.match('2[0-9]{3]',x['COMMENTAIR']) if not pd.isnull(x['COMMENTAIR']) else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='002': \n",
    "            dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['NatureCouc']) if not pd.isnull(x['NatureCouc']) else None, axis=1)\n",
    "            dfFichierSource['anneePose']=dfFichierSource.apply(lambda x : str(x.AnneeCouch) if not x.AnneeCouch[0]=='<' else str(x.AnneeCouch)[1:], axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',t.nom_attr_trafic:'trafic',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_granulo:'granulo'})\n",
    "        elif t.dept=='025' :\n",
    "            dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TMJA'] if x['%_PL']!='D' else None, axis=1)\n",
    "            dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['%_PL']) if x['%_PL']!='D' else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='027' :\n",
    "            dfFichierSource['pcpl']=dfFichierSource.F_PL.apply(lambda x : x.replace('%','').replace(',','.') if not pd.isnull(x) else None)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',t.nom_attr_route:'nomRoute',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='031' :\n",
    "            dfFichierSource['nomRoute']=dfFichierSource.route.apply(lambda x : O.epurationNomRoute(x))\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='037' :\n",
    "            if typeAtr=='vts' :\n",
    "                dfFichierSource['nomRoute']=dfFichierSource.axe.apply(lambda x : x.split('_')[1])\n",
    "            elif  typeAtr=='rvt' :\n",
    "                dfFichierSource['nomRoute']=dfFichierSource.IDROUTE.apply(lambda x : x.split('_')[1])\n",
    "                dfFichierSource['rvtType']=dfFichierSource.CG_TYPECOU.apply(lambda x : defRvt(x))\n",
    "            else :\n",
    "                dfFichierSource['nomRoute']=dfFichierSource.idroute.apply(lambda x : x.split('_')[1])\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='046': \n",
    "            dfFichierSource['trafic']=dfFichierSource['MJA TV (an'].apply(lambda x : x.replace(' ','').split('(')[0] if not pd.isnull(x) else None)\n",
    "            dfFichierSource['annee']=dfFichierSource['MJA TV (an'].apply(lambda x : x.replace(' ','').split('(')[1][:-1] if not pd.isnull(x) and '(' in x else None)\n",
    "            dfFichierSource['pcpl']=dfFichierSource['%PL'].apply(lambda x : float(x)*100 if not pd.isnull(x) else None)\n",
    "            dfFichierSource['nomRoute']=dfFichierSource['RD'].apply(lambda x : x.replace(' ','').replace('RD','D'))\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='049' :\n",
    "            dfFichierSource['nomRoute']=dfFichierSource.Route.apply(lambda x : O.epurationNomRoute(x[3:]))\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='052' :\n",
    "            dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['MJA_PL']/x['MJA_TV']*100) if not pd.isnull(x['MJA_TV']) else None, axis=1)\n",
    "            dfFichierSource['nomRoute']=dfFichierSource.Route.apply(lambda x : O.epurationNomRoute(x))\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='058' :\n",
    "            dfFichierSource['trafic']=dfFichierSource.apply(lambda x : x['TRAFFIC_VL']+x['TRAFFIC_PL'], axis=1)\n",
    "            dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['TRAFFIC_PL']/(x['TRAFFIC_VL']+x['TRAFFIC_PL'])*100) if x['TRAFFIC_VL']+x['TRAFFIC_PL']>0 else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='059' :\n",
    "            dfFichierSource['nomRoute']=dfFichierSource.VOIE.apply(lambda x : O.epurationNomRoute(x[1:]))\n",
    "            dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['TYP_DE_REV']) if not pd.isnull(x['TYP_DE_REV']) else defRvt(x['C_D_C_DE_S']), axis=1)\n",
    "            dfFichierSource['granulo']=dfFichierSource.apply(lambda x : re.match(r'([0-9]{1,2}/[0-9]{1,2})',x.GRANULOMET)[0] if not pd.isnull(x.GRANULOMET) and re.match(r'([0-9]/[0-9])',x.GRANULOMET) else None, axis=1)\n",
    "            dfFichierSource['anneePose']=dfFichierSource.apply(lambda x : str(x.D_D_C_DE_S)[:4] if not pd.isnull(x.D_D_C_DE_S) else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl'})\n",
    "        elif t.dept=='060' :\n",
    "            if  typeAtr=='trafic' :\n",
    "                dfFichierSource['pcpl']=dfFichierSource.PCT_PL_SEM.apply(lambda x : x.replace('%PL','') if not x=='%PL' else None)\n",
    "                dfFichierSource['annee']=dfFichierSource.DATE_COMPT.str[-4:]\n",
    "            elif typeAtr=='rvt': \n",
    "                dfFichierSource['rvtType']=dfFichierSource.NATURE_C_R.apply(lambda x : defRvt(x.split('_')[0] if not pd.isnull(x) else None))\n",
    "                dfFichierSource['granulo']=dfFichierSource.apply(lambda x : re.match(r'([0-9]{1,2}/[0-9]{1,2})',x.NATURE_C_R)[0] if not pd.isnull(x.NATURE_C_R) and re.match(r'([0-9]/[0-9])',x.NATURE_C_R) else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_route:'nomRoute',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        if t.dept=='067' :\n",
    "            if typeAtr=='rvt': \n",
    "                dfFichierSource['rvtType']=dfFichierSource.REVET.apply(lambda x : defRvt(x))\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_route:'nomRoute',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_annee_pose : 'anneePose',t.nom_attr_revetement:'granulo'}) \n",
    "        if t.dept=='068' : \n",
    "            if typeAtr=='rvt': \n",
    "                dfFichierSource['rvtType']=dfFichierSource.ROULEMENT_.apply(lambda x : defRvt(x))\n",
    "                dfFichierSource['granulo']=dfFichierSource.apply(lambda x : re.match(r'([0-9]{1,2}/[0-9]{1,2})',x.ROULEMEN_1)[0] if not pd.isnull(x.ROULEMEN_1) and re.match(r'([0-9]/[0-9])',x.ROULEMEN_1) else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_route:'nomRoute',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_annee_pose : 'anneePose'})             \n",
    "        elif t.dept=='070':\n",
    "            for c in [f'TA_TV{i}' for i in range(10,19)]: \n",
    "                dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c])>0 if not pd.isnull(x[c]) else False, axis=1),'trafic']= dfFichierSource[c]\n",
    "                dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1),'pcpl']=(dfFichierSource.loc[dfFichierSource.apply(\n",
    "                    lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1)].apply(lambda x : float(x[c.replace('TV','PL')]), axis=1)/\n",
    "                  dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c.replace('TV','PL')])>0 if not pd.isnull(x[c.replace('TV','PL')]) else False, axis=1)].apply(lambda x : float(x[c]), axis=1)*100)                \n",
    "                dfFichierSource.loc[dfFichierSource.apply(lambda x : float(x[c])>0 if not pd.isnull(x[c]) else False , axis=1),'annee']= f'20{c[-2:]}'\n",
    "                dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_route:'nomRoute',\n",
    "                                                                t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='075' :\n",
    "            dfFichierSource['trafic']=dfFichierSource.apply(lambda x : (x['MT']*12)+(x['ME']*4)+(x['MN']*8) if not (pd.isnull(x['MT']) and pd.isnull(x['ME']) and pd.isnull(x['MN'])) else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept =='080' :\n",
    "            dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].str[3:]\n",
    "            dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x['NATURE']) if not pd.isnull(x['NATURE']) else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='082' :\n",
    "            dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].str[3:]\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept =='076' : \n",
    "            dfFichierSource['nomRoute']=dfFichierSource[t.nom_attr_route].apply(lambda x : O.epurationNomRoute(x[3:]) if x[:2]!='VC' else x )\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='077' :\n",
    "            dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : str(x['PLTMJA']/x['TVTMJA']*100) if not pd.isnull(x['TVTMJA']) else None, axis=1)\n",
    "            dfFichierSource['nomRoute']=dfFichierSource.ROUTE.apply(lambda x : O.epurationNomRoute(x[2:]))\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.dept=='083' :\n",
    "            dfFichierSource['nomRoute']=dfFichierSource.ROUTE.apply(lambda x : O.epurationNomRoute(x[3:]))\n",
    "            if typeAtr=='vts' :\n",
    "                dfFichierSource['vtsvl']=dfFichierSource.PARAMETRES.apply(lambda x : float(x[10:12]) if not pd.isnull(x) else None)\n",
    "            elif typeAtr=='trafic' :        \n",
    "                dfFichierSource['trafic']=dfFichierSource.apply(lambda x : int(x['MJA_2018']) if int(x['MJA_2018'])>0 else int(x['MJA_2017']), axis=1)\n",
    "                dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : x['PL_2018'] if int(x['MJA_2018'])>0 else str(int(x['PL_2017'])/10), axis=1)\n",
    "                dfFichierSource['annee']=dfFichierSource.apply(lambda x : '2018' if int(x['MJA_2018'])>0 else '2017', axis=1)       \n",
    "            else : \n",
    "                dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x.MATERIAU) if not pd.isnull(x.MATERIAU) else defRvt(x.REVETEMENT), axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "    else : \n",
    "        if t.Agglo=='Henin-Carvin' : \n",
    "            dfFichierSource['nomRoute']=dfFichierSource.apply(lambda x : x['NUMERO'] if (not pd.isnull(x['NUMERO'])) and x['NUMERO']!='NC' else x['NOM_RUE_G'], axis=1)\n",
    "            dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : x['TMJA_PL']/x['TMJA_TOT']*100 if x['TMJA_TOT']!=0 else 0, axis=1)\n",
    "            dfFichierSource['annee']=dfFichierSource.DATDONNEES.apply(lambda x : f'20{x[-2:]}' if not pd.isnull(x) and re.match('[0-9]{2}',x[-2:])   else None)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', \n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.Agglo=='Colmar' : \n",
    "            dfFichierSource['annee']=dfFichierSource.Annee.apply(lambda x : str(x)[:4])\n",
    "            dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : x['PL_DMJJO']/x['TV_DMJJO']*100 if not pd.isnull(x['TV_DMJJO']) else None, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic', t.nom_attr_route:'nomRoute',\n",
    "                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'})\n",
    "        elif t.Agglo=='Lille' :\n",
    "            dfFichierSource['rvtType']=dfFichierSource.MATERIAUX.apply(lambda x : defRvt(x))\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_route:'nomRoute',t.nom_attr_pcpl:'pcpl', t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_annee_pose : 'anneePose',t.nom_attr_revetement:'granulo'})\n",
    "        elif t.Agglo=='Nantes' :\n",
    "            if typeAtr=='rvt' :\n",
    "                dfFichierSource['rvtType']=dfFichierSource.apply(lambda x : defRvt(x.REVETEMENT) if not pd.isnull(x.REVETEMENT) else None, axis=1)\n",
    "                dfFichierSource['nomRoute']=dfFichierSource.apply(lambda x : x['LIBELLE'] if not pd.isnull(x['LIBELLE']) else None, axis=1)\n",
    "            elif typeAtr=='trafic' :\n",
    "                dfFichierSource['pcpl']=dfFichierSource.apply(lambda x : x['Trafic_j_1']/x['Trafic_jou']*100 if x['Trafic_jou']!=0 else 0, axis=1)\n",
    "            dfFichierRenomme=dfFichierSource.rename(columns={t.nom_attr_trafic:'trafic',t.nom_attr_annee:'annee',\n",
    "                                                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_annee_pose : 'anneePose',t.nom_attr_revetement:'granulo'})\n",
    "    return dfFichierRenomme[listAttrNonNull]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Concat des donnees \n",
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('027') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #print(listAttrNonNull)\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('012','027', '037', '049', '052', '058', '059', '060', '075', '077','082', '083', '076','080'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcat=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'D:\\Boulot\\AffairesEnCours\\plamade\\RDs\\concat\\RD_concatenation_trafic_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.2 Principe mise en forme des fichiers ponctuels\n",
    ">> on va garder la structure précédente, mais il va aussi falloir ajouter les 2 jeu de données en tableur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide']=='oui') & (dfFichierSynthese['type']=='sig') & (dfFichierSynthese['type_geom']=='point')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('046') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('025','031', '070', '046'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "#tagguer les geoms nulles \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "dfConcat.loc[dfConcat.nomsource=='CD_008', 'annee']='2018'\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)\n",
    "\n",
    "#toutes les geoms en multi\n",
    "dfConcat[\"geometry\"] = [MultiPoint([feature]) if isinstance(feature, Point) else feature for feature in dfConcat[\"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'D:\\Boulot\\AffairesEnCours\\plamade\\RDs\\concat\\RD_concatenation_trafic_ponctuel.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Générer un fichier unique RD  \n",
    "sur la base de ce qui a été fait au dessus, on ajouter le revetement et la vitesse quand ils sont disponible au sein du même fichier source, pour les lignes puis les points.  \n",
    "ensuite il faut voir pour les fihiers multiples, on va plutot essayer de regrouper l'info au sein d'un seul fichier prétraité qui regroupera toute les infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.1 fichier lignes avec un seul fichier source et tous les attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non') & (dfFichierSynthese['fichier_unique']=='oui')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non') & (dfFichierSynthese['fichier_unique']=='oui')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['fichier_unique']=='oui')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012\n",
      "013\n",
      "027\n",
      "035\n",
      "036\n",
      "044\n",
      "045\n",
      "048\n",
      "049\n",
      "050\n",
      "052\n",
      "058\n",
      "059\n",
      "066\n",
      "071\n",
      "075\n",
      "076\n",
      "077\n",
      "080\n",
      "082\n"
     ]
    }
   ],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        if t.dept  in  ('037','067', '068','083') : \n",
    "                continue\n",
    "        for f in files : \n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',\n",
    "                             t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('012','027', '037', '049', '052', '058', '059', '060', '075', '077','082', '083', '076','080'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)\n",
    "\n",
    "#passage des NaN sur rvtType, granulo, anneePose en None\n",
    "dfConcat.loc[pd.isnull(dfConcat.rvtType), 'rvtType']=None\n",
    "dfConcat.loc[pd.isnull(dfConcat.granulo), 'granulo']=None\n",
    "dfConcat.loc[pd.isnull(dfConcat.anneePose), 'anneePose']=None\n",
    "\n",
    "#coller la granulo sur les codes Enumeres\n",
    "def granuloStandard(granulo) : \n",
    "    dico_granulo={'0/4' : ['0/4',],\n",
    "                    '0/6' : ['0/6',],\n",
    "                    '0/8' : ['0/8',],\n",
    "                    '0/10 type 1' : ['0/10','0/10 type 1'],\n",
    "                    '0/10 type 2' : ['0/10 type 2',],\n",
    "                    '0/14' : ['0/14',],\n",
    "                    '4/6' : ['4/6',],\n",
    "                    '6/8' : ['6/8','5/8'],\n",
    "                    '6/10' : ['6/10'],\n",
    "                    '10/14': ['10/14']}\n",
    "    if granulo : \n",
    "        granuloOk=[k for k, v in dico_granulo.items() if granulo in v][0]\n",
    "    else : \n",
    "        return None\n",
    "    return granuloOk\n",
    "dfConcat['granulo']=dfConcat.granulo.apply(lambda x : granuloStandard(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, '2017', '2014', '2009', '2004', '1990', '2011', '2012',\n",
       "       '2016', '2015', '2007', '2005', '2010', '2018', '2000', '2008',\n",
       "       '2002', '2006', '1997', '2003', '2019', '1984', '2001', '1998',\n",
       "       '1995', '1999', '2013', '1994', '1991', '1985', '1996', '1950',\n",
       "       '1978', '1987', '1993', '1960'], dtype=object)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple de verifs\n",
    "dfConcat.granulo.unique()\n",
    "dfConcat.anneePose.unique()\n",
    "#dfConcat.rvtType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Dossier_Perso\\Bruit\\PLAMADE\\Donnees_GT_flash\\donnees_produites\\Concat_RDs\\Concat_RDs_tousAttributs_1Fichier_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.2 fichier point avec un seul fichier source et tous les attributs (il n' y a pas de fichiers point multiples pour plusieurs attributs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='point')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002\n",
      "008\n",
      "025\n",
      "031\n",
      "039\n",
      "046\n",
      "070\n",
      "089\n"
     ]
    }
   ],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            \"\"\"if t.dept  not in  ('046') : \n",
    "                continue\"\"\"\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',\n",
    "                            t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('002','025','031', '070', '046'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)\n",
    "\n",
    "#passage des NaN sur rvtType, granulo, anneePose en None\n",
    "dfConcat.loc[pd.isnull(dfConcat.rvtType), 'rvtType']=None\n",
    "#dfConcat.loc[pd.isnull(dfConcat.granulo), 'granulo']=None\n",
    "dfConcat.loc[pd.isnull(dfConcat.anneePose), 'anneePose']=None\n",
    "\n",
    "#toutes les geoms en multi\n",
    "dfConcat[\"geometry\"] = [MultiPoint([feature]) if isinstance(feature, Point) else feature for feature in dfConcat[\"geometry\"]]\n",
    "\n",
    "#coller la granulo sur les codes Enumeres\n",
    "def granuloStandard(granulo) : \n",
    "    dico_granulo={'0/4' : ['0/4',],\n",
    "                    '0/6' : ['0/6',],\n",
    "                    '0/8' : ['0/8',],\n",
    "                    '0/10 type 1' : ['0/10','0/10 type 1'],\n",
    "                    '0/10 type 2' : ['0/10 type 2',],\n",
    "                    '0/14' : ['0/14',],\n",
    "                    '4/6' : ['4/6',],\n",
    "                    '6/8' : ['6/8','5/8'],\n",
    "                    '6/10' : ['6/10'],\n",
    "                    '10/14': ['10/14']}\n",
    "    if granulo : \n",
    "        granuloOk=[k for k, v in dico_granulo.items() if granulo in v][0]\n",
    "    else : \n",
    "        return None\n",
    "    return granuloOk\n",
    "#dfConcat['granulo']=dfConcat.granulo.apply(lambda x : granuloStandard(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BBSG', 'ES', nan], dtype=object)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple de verifs\n",
    "#dfConcatFixe.granulo.unique()\n",
    "#dfConcatFixe.anneePose.unique()\n",
    "dfConcat.rvtType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Dossier_Perso\\Bruit\\PLAMADE\\Donnees_GT_flash\\donnees_produites\\Concat_RDs\\Concat_RDs_tousAttributs_1Fichier_ponctuel.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.3 fichier lineaire avec un plusiuers fichier source ; attributs trafic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD'\n",
    "fichierSynthese=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\3-RD\\Synthese_type_fichier.ods'\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non') & (dfFichierSynthese['fichier_unique']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non') & (dfFichierSynthese['fichier_unique']=='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne') \n",
    "                                 & (dfFichierSynthese['fichier_unique']=='non')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "037\n",
      "060\n",
      "067\n",
      "068\n",
      "083\n"
     ]
    }
   ],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                #print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.dept in ('012','027', '037', '049', '052', '058', '059', '060', '075', '077','082', '083', '076','080'):\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull)\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Dossier_Perso\\Bruit\\PLAMADE\\Donnees_GT_flash\\donnees_produites\\Concat_RDs\\Concat_RDs_1Attributs_1Fichier_trafic_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.3 fichier lineaire avec un plusiuers fichier source ; attributs vitesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "037\n",
      "060\n",
      "067\n",
      "068\n",
      "083\n"
     ]
    }
   ],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            if isinstance(t.nom_fichier_vitesse, str) : \n",
    "                #print(f.lower(), t.nom_fichier_vitesse.lower())\n",
    "                if f.lower() == t.nom_fichier_vitesse.lower() : \n",
    "                    #print(f)\n",
    "                    #ouvrir et modifier le crs si besoin\n",
    "                    dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                    dicoNomAttr={t.nom_attr_routevitesse:'nomRoute',t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl'}\n",
    "                    listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                    #print(listAttrNonNull)\n",
    "                    #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                    if t.dept in ('037','083'):\n",
    "                        dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull, ('vts'))\n",
    "                    else :\n",
    "                        dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                    ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "#ajout attribut vts pl pour homogénéïté\n",
    "dfConcat['vtspl']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemple de vérif\n",
    "dfConcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Dossier_Perso\\Bruit\\PLAMADE\\Donnees_GT_flash\\donnees_produites\\Concat_RDs\\Concat_RDs_1Attributs_1Fichier_trafic_vitesse.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.3 fichier lineaire avec un plusiuers fichier source ; attributs revetement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "037\n",
      "['nomRoute', 'rvtType', 'granulo', 'anneePose', 'geometry'] {'IDROUTE': 'nomRoute', 'CG_TYPECOU': 'rvtType', 'CG_TYPEG_1': 'granulo', 'CGDT_MODIF': 'anneePose'}\n",
      "060\n",
      "['nomRoute', 'rvtType', 'geometry'] {'AXE': 'nomRoute', 'NATURE_C_R': 'rvtType', nan: 'anneePose'}\n",
      "067\n",
      "['nomRoute', 'rvtType', 'geometry'] {'AXE': 'nomRoute', 'REVET': 'rvtType', nan: 'anneePose'}\n",
      "068\n",
      "['nomRoute', 'rvtType', 'granulo', 'anneePose', 'geometry'] {'AXE': 'nomRoute', 'ROULEMENT_': 'rvtType', 'ROULEMEN_1': 'granulo', 'ANNEE': 'anneePose'}\n",
      "083\n",
      "['nomRoute', 'rvtType', 'anneePose', 'geometry'] {'ROUTE': 'nomRoute', 'REVETEMENT, MATERIAU': 'rvtType', nan: 'granulo', 'ANNEE': 'anneePose'}\n"
     ]
    }
   ],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.dept)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.dept)) : \n",
    "        for f in files : \n",
    "            if isinstance(t.nom_fichier_revetement, str) : \n",
    "                #print(f.lower(), t.nom_fichier_vitesse.lower())\n",
    "                if f.lower() == t.nom_fichier_revetement.lower() : \n",
    "                    #print(f)\n",
    "                    #ouvrir et modifier le crs si besoin\n",
    "                    dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                    dicoNomAttr={t.nom_attr_route_rvt:'nomRoute',t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                    listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                    print(listAttrNonNull,dicoNomAttr )\n",
    "                    #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                    if t.dept in ('037','060','067','068','083',):\n",
    "                        dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull, ('rvt'))\n",
    "                    else :\n",
    "                        dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                    ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.dept]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.anneePose=dfConcat.anneePose.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.anneePose=dfConcat.anneePose.str[:4]\n",
    "dfConcat.loc[dfConcat.anneePose.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#passage des NaN sur rvtType, granulo, anneePose en None\n",
    "dfConcat.loc[pd.isnull(dfConcat.rvtType), 'rvtType']=None\n",
    "dfConcat.loc[pd.isnull(dfConcat.granulo), 'granulo']=None\n",
    "dfConcat.loc[pd.isnull(dfConcat.anneePose) | (dfConcat.anneePose=='0'), 'anneePose']=None\n",
    "\n",
    "#coller la granulo sur les codes Enumeres\n",
    "def granuloStandard(granulo) : \n",
    "    dico_granulo={'0/4' : ['0/4','2/4'],\n",
    "                    '0/6' : ['0/6','0/5'],\n",
    "                    '0/8' : ['0/8',],\n",
    "                    '0/10 type 1' : ['0/10','0/10 type 1', '-', '10/1','/100', '0/25'],\n",
    "                    '0/10 type 2' : ['0/10 type 2',],\n",
    "                    '0/14' : ['0/14','0/20', '0/31'],\n",
    "                    '4/6' : ['4/6', '6/4', '4/06'],\n",
    "                    '6/8' : ['6/8','5/8'],\n",
    "                    '6/10' : ['6/10','4/10'],\n",
    "                    '10/14': ['10/14']}\n",
    "    try :\n",
    "        if granulo : \n",
    "            granuloOk=[k for k, v in dico_granulo.items() if granulo in v][0]\n",
    "        else : \n",
    "            return None\n",
    "    except IndexError : \n",
    "        print(granulo)\n",
    "    return granuloOk\n",
    "dfConcat['granulo']=dfConcat.granulo.apply(lambda x : granuloStandard(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, '0/10 type 1', '4/6', '0/6', '6/10', '10/14', '0/4', '0/14',\n",
       "       '0/8', '6/8'], dtype=object)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple verifs\n",
    "dfConcat.rvtType.unique()\n",
    "dfConcat.granulo.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Dossier_Perso\\Bruit\\PLAMADE\\Donnees_GT_flash\\donnees_produites\\Concat_RDs\\Concat_RDs_1Attributs_1Fichier_Revetement_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Générer un fichier unique Agglo  \n",
    "sur la base de ce qui a été fait au dessus, on va générer un fichier agglo  \n",
    "**ATTENTION : cas particulier sur les agglos de Montpellier et Saint-Quentin en Yvelines :** \n",
    "- Montpellier est issu de modele de trafic : il va falloir regrouper les lignes identiques pour avoir le trafic 2 sens confondu, et gérer les bouble voie\n",
    "- Saint-Quentin est en portée chaussé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\old'\n",
    "fichierSynthese=os.path.join(dossierDonneesRd,'Synthese_type_fichier_agglo.ods')\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lister les agglos dispos facilement\n",
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non') & (dfFichierSynthese['fichier_unique']=='oui')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non') & (dfFichierSynthese['fichier_unique']=='oui')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['fichier_unique']=='oui') & (~dfFichierSynthese.Agglo.isin(('Montpellier','Saint-Quentin-en-Yveline'))) ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agglo</th>\n",
       "      <th>nom_fichier_trafic</th>\n",
       "      <th>valide_trafic</th>\n",
       "      <th>type_trafic</th>\n",
       "      <th>type_geom_trafic</th>\n",
       "      <th>nom_attr_trafic</th>\n",
       "      <th>nom_attr_pcpl</th>\n",
       "      <th>nom_attr_route</th>\n",
       "      <th>nom_attr_annee</th>\n",
       "      <th>projection</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_fichier_revetement</th>\n",
       "      <th>valide_revetement</th>\n",
       "      <th>nom_attr_route_rvt</th>\n",
       "      <th>type_revetement</th>\n",
       "      <th>type_geom_revetement</th>\n",
       "      <th>nom_attr_revetement</th>\n",
       "      <th>nom_attr_granulo</th>\n",
       "      <th>nom_attr_annee_pose</th>\n",
       "      <th>regles gestion_revetement</th>\n",
       "      <th>commentaires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perpignan</td>\n",
       "      <td>Perpignan_Cerema_ponctuel2lineaire.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>MJA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOM_VOIE</td>\n",
       "      <td>ANNEE</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Henin-Carvin</td>\n",
       "      <td>ROUTES_CAHC.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>TMJA_TOT</td>\n",
       "      <td>TMJA_PL, TMJA_TOT</td>\n",
       "      <td>NUMERO, NOM_RUE_G</td>\n",
       "      <td>DATDONNEES</td>\n",
       "      <td>Epsg:3950</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Besancon</td>\n",
       "      <td>Besancon_Cerema_ponctuel2lineaire.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>TMJA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Epsg:3947</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Blois</td>\n",
       "      <td>TRAFFIC_BDVOIRIE_Ville_de_BLOIS.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>GLOBAL_VEH</td>\n",
       "      <td>POURCENT_P</td>\n",
       "      <td>LIBELLE_PR</td>\n",
       "      <td>ANNEE</td>\n",
       "      <td>Epsg:3948</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Colmar</td>\n",
       "      <td>Comptage_link.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>TV_DMJJO</td>\n",
       "      <td>PL_DMJJO/TV_DMJJO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Annee</td>\n",
       "      <td>Epsg:3948</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lille</td>\n",
       "      <td>Sources routières - MEL.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>TMJA_TV</td>\n",
       "      <td>POURCENT_P</td>\n",
       "      <td>NOM_RUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>Lille</td>\n",
       "      <td>Sources routières – MEL.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>MATERIAUX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Morteau</td>\n",
       "      <td>Morteau_simplifie_Cerema.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>TMJA_TV</td>\n",
       "      <td>POURCENT_P</td>\n",
       "      <td>NOM_1_GAUC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Agglo                      nom_fichier_trafic valide_trafic  \\\n",
       "0     Perpignan  Perpignan_Cerema_ponctuel2lineaire.shp           oui   \n",
       "1  Henin-Carvin                         ROUTES_CAHC.shp           oui   \n",
       "4      Besancon   Besancon_Cerema_ponctuel2lineaire.shp           oui   \n",
       "5         Blois     TRAFFIC_BDVOIRIE_Ville_de_BLOIS.shp           oui   \n",
       "6        Colmar                       Comptage_link.shp           oui   \n",
       "8         Lille             Sources routières - MEL.shp           oui   \n",
       "9       Morteau            Morteau_simplifie_Cerema.shp           oui   \n",
       "\n",
       "  type_trafic type_geom_trafic nom_attr_trafic      nom_attr_pcpl  \\\n",
       "0         sig            ligne             MJA                NaN   \n",
       "1         sig            ligne        TMJA_TOT  TMJA_PL, TMJA_TOT   \n",
       "4         sig            ligne            TMJA                NaN   \n",
       "5         sig            ligne      GLOBAL_VEH         POURCENT_P   \n",
       "6         sig            ligne        TV_DMJJO  PL_DMJJO/TV_DMJJO   \n",
       "8         sig            ligne         TMJA_TV         POURCENT_P   \n",
       "9         sig            ligne         TMJA_TV         POURCENT_P   \n",
       "\n",
       "      nom_attr_route nom_attr_annee projection  ... nom_fichier_revetement  \\\n",
       "0           NOM_VOIE          ANNEE  Epsg:2154  ...                    NaN   \n",
       "1  NUMERO, NOM_RUE_G     DATDONNEES  Epsg:3950  ...                    NaN   \n",
       "4                NaN            NaN  Epsg:3947  ...                    NaN   \n",
       "5         LIBELLE_PR          ANNEE  Epsg:3948  ...                    NaN   \n",
       "6                NaN          Annee  Epsg:3948  ...                    NaN   \n",
       "8            NOM_RUE            NaN  Epsg:2154  ...                  Lille   \n",
       "9         NOM_1_GAUC            NaN  Epsg:2154  ...                    NaN   \n",
       "\n",
       "             valide_revetement nom_attr_route_rvt type_revetement  \\\n",
       "0                          NaN                NaN             NaN   \n",
       "1                          NaN                NaN             NaN   \n",
       "4                          NaN                NaN             NaN   \n",
       "5                          NaN                NaN             NaN   \n",
       "6                          NaN                NaN             NaN   \n",
       "8  Sources routières – MEL.shp                oui             sig   \n",
       "9                          NaN                NaN             NaN   \n",
       "\n",
       "  type_geom_revetement  nom_attr_revetement nom_attr_granulo  \\\n",
       "0                  NaN                  NaN              NaN   \n",
       "1                  NaN                  NaN              NaN   \n",
       "4                  NaN                  NaN              NaN   \n",
       "5                  NaN                  NaN              NaN   \n",
       "6                  NaN                  NaN              NaN   \n",
       "8                ligne            MATERIAUX              NaN   \n",
       "9                  NaN                  NaN              NaN   \n",
       "\n",
       "  nom_attr_annee_pose regles gestion_revetement commentaires  \n",
       "0                 NaN                       NaN          NaN  \n",
       "1                 NaN                       NaN          NaN  \n",
       "4                 NaN                       NaN          NaN  \n",
       "5                 NaN                       NaN          NaN  \n",
       "6                 NaN                       NaN          NaN  \n",
       "8                 NaN                       NaN          NaN  \n",
       "9                 NaN                       NaN          NaN  \n",
       "\n",
       "[7 rows x 30 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNettoyees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perpignan\n",
      "fiche_descriptive_perpignan.ods perpignan_cerema_ponctuel2lineaire.shp\n",
      "mapping_perpignan_cerema_ponctuel2lineaire.ods perpignan_cerema_ponctuel2lineaire.shp\n",
      "perpignan_cerema_ponctuel2lineaire.cpg perpignan_cerema_ponctuel2lineaire.shp\n",
      "perpignan_cerema_ponctuel2lineaire.dbf perpignan_cerema_ponctuel2lineaire.shp\n",
      "perpignan_cerema_ponctuel2lineaire.prj perpignan_cerema_ponctuel2lineaire.shp\n",
      "perpignan_cerema_ponctuel2lineaire.shp perpignan_cerema_ponctuel2lineaire.shp\n",
      "Perpignan_Cerema_ponctuel2lineaire.shp\n",
      "perpignan_cerema_ponctuel2lineaire.shx perpignan_cerema_ponctuel2lineaire.shp\n",
      "extrait_comptages_routiers_perpignanmm_02-04-2019.dbf perpignan_cerema_ponctuel2lineaire.shp\n",
      "extrait_comptages_routiers_perpignanmm_02-04-2019.prj perpignan_cerema_ponctuel2lineaire.shp\n",
      "extrait_comptages_routiers_perpignanmm_02-04-2019.qpj perpignan_cerema_ponctuel2lineaire.shp\n",
      "extrait_comptages_routiers_perpignanmm_02-04-2019.shp perpignan_cerema_ponctuel2lineaire.shp\n",
      "extrait_comptages_routiers_perpignanmm_02-04-2019.shx perpignan_cerema_ponctuel2lineaire.shp\n",
      "Henin-Carvin\n",
      "fiche_descriptive_henin-carvin.ods routes_cahc.shp\n",
      "mapping_routes_cahc.ods routes_cahc.shp\n",
      "routes_cahc.dbf routes_cahc.shp\n",
      "routes_cahc.shp routes_cahc.shp\n",
      "ROUTES_CAHC.shp\n",
      "agglo\n",
      "routes_cahc.shx routes_cahc.shp\n",
      "Besancon\n",
      "besancon_cerema_ponctuel2lineaire.cpg besancon_cerema_ponctuel2lineaire.shp\n",
      "besancon_cerema_ponctuel2lineaire.dbf besancon_cerema_ponctuel2lineaire.shp\n",
      "besancon_cerema_ponctuel2lineaire.prj besancon_cerema_ponctuel2lineaire.shp\n",
      "besancon_cerema_ponctuel2lineaire.shp besancon_cerema_ponctuel2lineaire.shp\n",
      "Besancon_Cerema_ponctuel2lineaire.shp\n",
      "besancon_cerema_ponctuel2lineaire.shx besancon_cerema_ponctuel2lineaire.shp\n",
      "fiche_descriptive_besancon.ods besancon_cerema_ponctuel2lineaire.shp\n",
      "mapping_besancon.ods besancon_cerema_ponctuel2lineaire.shp\n",
      "2020-01-06_besancon_tmjasup8200.cpg besancon_cerema_ponctuel2lineaire.shp\n",
      "2020-01-06_besancon_tmjasup8200.dbf besancon_cerema_ponctuel2lineaire.shp\n",
      "2020-01-06_besancon_tmjasup8200.prj besancon_cerema_ponctuel2lineaire.shp\n",
      "2020-01-06_besancon_tmjasup8200.qpj besancon_cerema_ponctuel2lineaire.shp\n",
      "2020-01-06_besancon_tmjasup8200.shp besancon_cerema_ponctuel2lineaire.shp\n",
      "2020-01-06_besancon_tmjasup8200.shx besancon_cerema_ponctuel2lineaire.shp\n",
      "Blois\n",
      "fiche_descriptive_blois.ods traffic_bdvoirie_ville_de_blois.shp\n",
      "mapping_blois.ods traffic_bdvoirie_ville_de_blois.shp\n",
      "traffic_bdvoirie_ville_de_blois.cpg traffic_bdvoirie_ville_de_blois.shp\n",
      "traffic_bdvoirie_ville_de_blois.dbf traffic_bdvoirie_ville_de_blois.shp\n",
      "traffic_bdvoirie_ville_de_blois.prj traffic_bdvoirie_ville_de_blois.shp\n",
      "traffic_bdvoirie_ville_de_blois.sbn traffic_bdvoirie_ville_de_blois.shp\n",
      "traffic_bdvoirie_ville_de_blois.sbx traffic_bdvoirie_ville_de_blois.shp\n",
      "traffic_bdvoirie_ville_de_blois.shp traffic_bdvoirie_ville_de_blois.shp\n",
      "TRAFFIC_BDVOIRIE_Ville_de_BLOIS.shp\n",
      "traffic_bdvoirie_ville_de_blois.shx traffic_bdvoirie_ville_de_blois.shp\n",
      "Colmar\n",
      "fiche_descriptive_colmar.ods comptage_link.shp\n",
      "mapping_colmar.ods comptage_link.shp\n",
      "enrobes.cpg comptage_link.shp\n",
      "enrobes.dbf comptage_link.shp\n",
      "enrobes.prj comptage_link.shp\n",
      "enrobes.qpj comptage_link.shp\n",
      "enrobes.shp comptage_link.shp\n",
      "enrobes.shx comptage_link.shp\n",
      "comptage_link.cpg comptage_link.shp\n",
      "comptage_link.ctf comptage_link.shp\n",
      "comptage_link.dbf comptage_link.shp\n",
      "comptage_link.prj comptage_link.shp\n",
      "comptage_link.shp comptage_link.shp\n",
      "Comptage_link.SHP\n",
      "agglo\n",
      "comptage_link.shx comptage_link.shp\n",
      "Lille\n",
      "fiche_descriptive_lille.ods sources routières - mel.shp\n",
      "mapping_sources routières - mel.ods sources routières - mel.shp\n",
      "sources routières - mel.dbf sources routières - mel.shp\n",
      "sources routières - mel.prj sources routières - mel.shp\n",
      "sources routières - mel.qpj sources routières - mel.shp\n",
      "sources routières - mel.sbn sources routières - mel.shp\n",
      "sources routières - mel.sbx sources routières - mel.shp\n",
      "sources routières - mel.shp sources routières - mel.shp\n",
      "Sources routières - MEL.shp\n",
      "agglo\n",
      "sources routières - mel.shx sources routières - mel.shp\n",
      "Morteau\n",
      "2017-06-14--trafic morteau.cpg morteau_simplifie_cerema.shp\n",
      "2017-06-14--trafic morteau.dbf morteau_simplifie_cerema.shp\n",
      "2017-06-14--trafic morteau.prj morteau_simplifie_cerema.shp\n",
      "2017-06-14--trafic morteau.qpj morteau_simplifie_cerema.shp\n",
      "2017-06-14--trafic morteau.shp morteau_simplifie_cerema.shp\n",
      "2017-06-14--trafic morteau.shx morteau_simplifie_cerema.shp\n",
      "fiche_descriptive_morteau.ods morteau_simplifie_cerema.shp\n",
      "mapping_2017-06-14--trafic morteau.ods morteau_simplifie_cerema.shp\n",
      "morteau_simplifie_cerema.cpg morteau_simplifie_cerema.shp\n",
      "morteau_simplifie_cerema.dbf morteau_simplifie_cerema.shp\n",
      "morteau_simplifie_cerema.prj morteau_simplifie_cerema.shp\n",
      "morteau_simplifie_cerema.shp morteau_simplifie_cerema.shp\n",
      "Morteau_simplifie_Cerema.shp\n",
      "morteau_simplifie_cerema.shx morteau_simplifie_cerema.shp\n"
     ]
    }
   ],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.Agglo)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.Agglo)) : \n",
    "        if t.Agglo  in  ('Nantes', 'Quimper') : \n",
    "                continue\n",
    "        for f in files : \n",
    "            print(f.lower(), t.nom_fichier_trafic.lower())\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee',\n",
    "                             t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl', t.nom_attr_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.Agglo in ('Henin-Carvin','Colmar', 'Lille'):\n",
    "                    print('agglo')\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull, typegest='agglo')\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.Agglo]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "#ajouter Saint-quentin-en-Yveline et montpellier et les ousrces RD sans le 044\n",
    "sourceRD=gp.read_file(r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\old\\ConcatRD\\Concat_RDs_tousAttributs_1Fichier_lineaire.shp')\n",
    "sourceRD=sourceRD.loc[sourceRD.nomsource!='CD_044'].copy()\n",
    "sqy=gp.read_file(r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\old\\Saint-Quentin-en-Yvelines\\Saint-Quentin_valide_final.shp', crs='epsg:3949')\n",
    "sqy=sqy.to_crs('epsg:2154')\n",
    "dfConcat=pd.concat([dfConcatFixe,\n",
    "           sqy,\n",
    "          gp.read_file(r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\old\\Montpellier\\Montpellier_trafic2sens_valide_final.shp').drop('id', axis=1),\n",
    "                   sourceRD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) and x!='nan' and x!='0.0' else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corriger les vitesse\n",
    "def modifVitesse(vts):\n",
    "    if pd.isnull(vts) or vts==0 : \n",
    "        return None\n",
    "    elif vts < 20 : \n",
    "        return 30\n",
    "    elif vts in (63, 67) :\n",
    "        return 70\n",
    "    elif vts in(92,): \n",
    "        return 90\n",
    "    elif vts in (58,45) : \n",
    "        return 50\n",
    "    else :\n",
    "        return vts\n",
    "dfConcat['vtsvl']=dfConcat.vtsvl.apply(lambda x : modifVitesse(x))\n",
    "dfConcat['vtspl']=dfConcat.vtspl.apply(lambda x : modifVitesse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passage des NaN sur rvtType, granulo, anneePose en None\n",
    "dfConcat.loc[pd.isnull(dfConcat.rvtType), 'rvtType']=None\n",
    "dfConcat.loc[pd.isnull(dfConcat.granulo), 'granulo']=None\n",
    "dfConcat.loc[pd.isnull(dfConcat.anneePose) | (dfConcat.anneePose=='0'), 'anneePose']=None\n",
    "\n",
    "#coller la granulo sur les codes Enumeres\n",
    "def granuloStandard(granulo) : \n",
    "    dico_granulo={'0/4' : ['0/4','2/4'],\n",
    "                    '0/6' : ['0/6','0/5'],\n",
    "                    '0/8' : ['0/8',],\n",
    "                    '0/10 type 1' : ['0/10','0/10 type 1', '-', '10/1','/100', '0/25'],\n",
    "                    '0/10 type 2' : ['0/10 type 2',],\n",
    "                    '0/14' : ['0/14','0/20', '0/31'],\n",
    "                    '4/6' : ['4/6', '6/4', '4/06'],\n",
    "                    '6/8' : ['6/8','5/8'],\n",
    "                    '6/10' : ['6/10','4/10'],\n",
    "                    '10/14': ['10/14']}\n",
    "    try :\n",
    "        if granulo : \n",
    "            granuloOk=[k for k, v in dico_granulo.items() if granulo in v][0]\n",
    "        else : \n",
    "            return None\n",
    "    except IndexError : \n",
    "        print(granulo)\n",
    "    return granuloOk\n",
    "dfConcat['granulo']=dfConcat.granulo.apply(lambda x : granuloStandard(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10741.,  9404.,  8885., ...,  4606., 17133., 28054.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfConcat.trafic.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Commune_Perpignan', 'Commune_Henin-Carvin', 'Commune_Besancon',\n",
       "       'Commune_Blois', 'Commune_Colmar', 'Commune_Lille',\n",
       "       'Commune_Morteau', 'Commune_Saint-Quentin-en-Yvelines',\n",
       "       'Metropole_Montpellier', 'CD_012', 'CD_035', 'CD_052', 'CD_058',\n",
       "       'CD_066', 'CD_075', 'CD_076'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfConcat.nomsource.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfConcat.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\0_ConcatAGglos\\ConCatAgglo_tousattribut_1fichier.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.2 fichier de trafic pour 1 fichier lineaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\old'\n",
    "fichierSynthese=os.path.join(dossierDonneesRd,'Synthese_type_fichier_agglo.ods')\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non') & (dfFichierSynthese['fichier_unique']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non') & (dfFichierSynthese['fichier_unique']=='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne') \n",
    "                                 & (dfFichierSynthese['fichier_unique']=='non')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agglo</th>\n",
       "      <th>nom_fichier_trafic</th>\n",
       "      <th>valide_trafic</th>\n",
       "      <th>type_trafic</th>\n",
       "      <th>type_geom_trafic</th>\n",
       "      <th>nom_attr_trafic</th>\n",
       "      <th>nom_attr_pcpl</th>\n",
       "      <th>nom_attr_route</th>\n",
       "      <th>nom_attr_annee</th>\n",
       "      <th>projection</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_fichier_revetement</th>\n",
       "      <th>valide_revetement</th>\n",
       "      <th>nom_attr_route_rvt</th>\n",
       "      <th>type_revetement</th>\n",
       "      <th>type_geom_revetement</th>\n",
       "      <th>nom_attr_revetement</th>\n",
       "      <th>nom_attr_granulo</th>\n",
       "      <th>nom_attr_annee_pose</th>\n",
       "      <th>regles gestion_revetement</th>\n",
       "      <th>commentaires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quimper</td>\n",
       "      <td>Cerema_Quimper_trafic_ligne.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOM</td>\n",
       "      <td>ANNEE_CPTG</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nantes</td>\n",
       "      <td>TraficGeoNantes2018.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>Trafic_jou</td>\n",
       "      <td>Trafic_j_1,Trafic_jou</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Epsg:3947</td>\n",
       "      <td>...</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Agglo               nom_fichier_trafic valide_trafic type_trafic  \\\n",
       "3  Quimper  Cerema_Quimper_trafic_ligne.shp           oui         sig   \n",
       "7   Nantes          TraficGeoNantes2018.shp           oui         sig   \n",
       "\n",
       "  type_geom_trafic nom_attr_trafic          nom_attr_pcpl nom_attr_route  \\\n",
       "3            ligne              TV                    NaN            NOM   \n",
       "7            ligne      Trafic_jou  Trafic_j_1,Trafic_jou            NaN   \n",
       "\n",
       "  nom_attr_annee projection  ... nom_fichier_revetement valide_revetement  \\\n",
       "3     ANNEE_CPTG  Epsg:2154  ...                    NaN               NaN   \n",
       "7            NaN  Epsg:3947  ...                 Nantes               NaN   \n",
       "\n",
       "  nom_attr_route_rvt type_revetement type_geom_revetement  \\\n",
       "3                NaN             NaN                  NaN   \n",
       "7                NaN             NaN                  NaN   \n",
       "\n",
       "   nom_attr_revetement nom_attr_granulo nom_attr_annee_pose  \\\n",
       "3                  NaN              NaN                 NaN   \n",
       "7                  NaN              NaN                 NaN   \n",
       "\n",
       "  regles gestion_revetement commentaires  \n",
       "3                       NaN          NaN  \n",
       "7                       NaN          NaN  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNettoyees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.Agglo)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.Agglo)) : \n",
    "        if t.Agglo  not in  ('Nantes', 'Quimper') : \n",
    "                continue\n",
    "        for f in files : \n",
    "            print(f.lower(), t.nom_fichier_trafic.lower())\n",
    "            if f.lower() == t.nom_fichier_trafic.lower() : \n",
    "                print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_trafic:'trafic', t.nom_attr_pcpl:'pcpl', t.nom_attr_route:'nomRoute',t.nom_attr_annee:'annee'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                if t.Agglo in ('Nantes'):\n",
    "                    print('agglo')\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull, typegest='agglo')\n",
    "                else :\n",
    "                    dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.Agglo]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitemnets avant export : \n",
    "dfConcat=dfConcatFixe.copy()\n",
    "#tagguer les geoms nulles \n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : not pd.isnull(x)), 'geom_nulle']=False\n",
    "dfConcat.loc[dfConcat.geometry.apply(lambda x : pd.isnull(x)), 'geom_nulle']=True\n",
    "\n",
    "#corriger des valeurs louches sur les annees et passer en format date AAAA pour le reste\n",
    "dfConcat.annee=dfConcat.annee.apply(lambda x : str(x) if not pd.isnull(x) else None)\n",
    "dfConcat.annee=dfConcat.annee.str[:4]\n",
    "dfConcat.loc[dfConcat.annee.apply(lambda x : float(x)<1950 if not (pd.isnull(x)) else True), 'annee']=None\n",
    "\n",
    "#mise en forme valeurs TMJA et pcpl : \n",
    "dfConcat.trafic=dfConcat.trafic.apply(lambda x : round(float(x),0) if not pd.isnull(x) else np.nan)\n",
    "dfConcat.pcpl=dfConcat.pcpl.apply(lambda x : round(float(str(x).replace(',','.')),2) if not pd.isnull(x) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\0_ConcatAGglos\\Concat_Agglo_1Attributs_1Fichier_trafic_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.2 fichier de vitesse pour 1 fichier lineaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\old'\n",
    "fichierSynthese=os.path.join(dossierDonneesRd,'Synthese_type_fichier_agglo.ods')\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non') & (dfFichierSynthese['fichier_unique']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non') & (dfFichierSynthese['fichier_unique']=='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne') \n",
    "                                 & (dfFichierSynthese['fichier_unique']=='non')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agglo</th>\n",
       "      <th>nom_fichier_trafic</th>\n",
       "      <th>valide_trafic</th>\n",
       "      <th>type_trafic</th>\n",
       "      <th>type_geom_trafic</th>\n",
       "      <th>nom_attr_trafic</th>\n",
       "      <th>nom_attr_pcpl</th>\n",
       "      <th>nom_attr_route</th>\n",
       "      <th>nom_attr_annee</th>\n",
       "      <th>projection</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_fichier_revetement</th>\n",
       "      <th>valide_revetement</th>\n",
       "      <th>nom_attr_route_rvt</th>\n",
       "      <th>type_revetement</th>\n",
       "      <th>type_geom_revetement</th>\n",
       "      <th>nom_attr_revetement</th>\n",
       "      <th>nom_attr_granulo</th>\n",
       "      <th>nom_attr_annee_pose</th>\n",
       "      <th>regles gestion_revetement</th>\n",
       "      <th>commentaires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quimper</td>\n",
       "      <td>Cerema_Quimper_trafic_ligne.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOM</td>\n",
       "      <td>ANNEE_CPTG</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nantes</td>\n",
       "      <td>TraficGeoNantes2018.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>Trafic_jou</td>\n",
       "      <td>Trafic_j_1,Trafic_jou</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Epsg:3947</td>\n",
       "      <td>...</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Agglo               nom_fichier_trafic valide_trafic type_trafic  \\\n",
       "3  Quimper  Cerema_Quimper_trafic_ligne.shp           oui         sig   \n",
       "7   Nantes          TraficGeoNantes2018.shp           oui         sig   \n",
       "\n",
       "  type_geom_trafic nom_attr_trafic          nom_attr_pcpl nom_attr_route  \\\n",
       "3            ligne              TV                    NaN            NOM   \n",
       "7            ligne      Trafic_jou  Trafic_j_1,Trafic_jou            NaN   \n",
       "\n",
       "  nom_attr_annee projection  ... nom_fichier_revetement valide_revetement  \\\n",
       "3     ANNEE_CPTG  Epsg:2154  ...                    NaN               NaN   \n",
       "7            NaN  Epsg:3947  ...                 Nantes               NaN   \n",
       "\n",
       "  nom_attr_route_rvt type_revetement type_geom_revetement  \\\n",
       "3                NaN             NaN                  NaN   \n",
       "7                NaN             NaN                  NaN   \n",
       "\n",
       "   nom_attr_revetement nom_attr_granulo nom_attr_annee_pose  \\\n",
       "3                  NaN              NaN                 NaN   \n",
       "7                  NaN              NaN                 NaN   \n",
       "\n",
       "  regles gestion_revetement commentaires  \n",
       "3                       NaN          NaN  \n",
       "7                       NaN          NaN  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNettoyees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quimper\n",
      "cerema_quimper_trafic_ligne.cpg vitesses_2019.shp\n",
      "cerema_quimper_trafic_ligne.dbf vitesses_2019.shp\n",
      "cerema_quimper_trafic_ligne.prj vitesses_2019.shp\n",
      "cerema_quimper_trafic_ligne.shp vitesses_2019.shp\n",
      "cerema_quimper_trafic_ligne.shx vitesses_2019.shp\n",
      "fiche_descriptive_quimper.ods vitesses_2019.shp\n",
      "mapping_quimper.ods vitesses_2019.shp\n",
      "cptg_tab_quimper.cpg vitesses_2019.shp\n",
      "cptg_tab_quimper.dbf vitesses_2019.shp\n",
      "cptg_tab_quimper.dbf.xml vitesses_2019.shp\n",
      "cptg_tab_quimper.n°compteu.atx vitesses_2019.shp\n",
      "sitecomptage.cpg vitesses_2019.shp\n",
      "sitecomptage.dbf vitesses_2019.shp\n",
      "sitecomptage.prj vitesses_2019.shp\n",
      "sitecomptage.sbn vitesses_2019.shp\n",
      "sitecomptage.sbx vitesses_2019.shp\n",
      "sitecomptage.shp vitesses_2019.shp\n",
      "sitecomptage.shp.xml vitesses_2019.shp\n",
      "sitecomptage.shx vitesses_2019.shp\n",
      "voies.cpg vitesses_2019.shp\n",
      "voies.dbf vitesses_2019.shp\n",
      "voies.prj vitesses_2019.shp\n",
      "voies.sbn vitesses_2019.shp\n",
      "voies.sbx vitesses_2019.shp\n",
      "voies.shp vitesses_2019.shp\n",
      "voies.shp.xml vitesses_2019.shp\n",
      "voies.shx vitesses_2019.shp\n",
      "vitesses_2019.cpg vitesses_2019.shp\n",
      "vitesses_2019.dbf vitesses_2019.shp\n",
      "vitesses_2019.prj vitesses_2019.shp\n",
      "vitesses_2019.sbn vitesses_2019.shp\n",
      "vitesses_2019.sbx vitesses_2019.shp\n",
      "vitesses_2019.shp vitesses_2019.shp\n",
      "Vitesses_2019.shp\n",
      "vitesses_2019.shx vitesses_2019.shp\n",
      "Nantes\n",
      ".~lock.fiche_descriptive_nantes.ods# vitesse.shp\n",
      "fiche_descriptive_nantes.ods vitesse.shp\n",
      "mapping_traficgeonantes2018.ods vitesse.shp\n",
      "mapping_vitesse.ods vitesse.shp\n",
      "revetement_route.cpg vitesse.shp\n",
      "revetement_route.prj vitesse.shp\n",
      "revetement_route.qpj vitesse.shp\n",
      "revetement_route.shp vitesse.shp\n",
      "revetement_route.shx vitesse.shp\n",
      "traficgeonantes2018.cpg vitesse.shp\n",
      "traficgeonantes2018.dbf vitesse.shp\n",
      "traficgeonantes2018.prj vitesse.shp\n",
      "traficgeonantes2018.qpj vitesse.shp\n",
      "traficgeonantes2018.shp vitesse.shp\n",
      "traficgeonantes2018.shx vitesse.shp\n",
      "vitesse.cpg vitesse.shp\n",
      "vitesse.dbf vitesse.shp\n",
      "vitesse.prj vitesse.shp\n",
      "vitesse.qpj vitesse.shp\n",
      "vitesse.shp vitesse.shp\n",
      "Vitesse.shp\n",
      "vitesse.shx vitesse.shp\n",
      "circul_agglomeration_s.dat vitesse.shp\n",
      "circul_agglomeration_s.id vitesse.shp\n",
      "circul_agglomeration_s.map vitesse.shp\n",
      "circul_agglomeration_s.tab vitesse.shp\n",
      "circul_giratoire_p.dat vitesse.shp\n",
      "circul_giratoire_p.id vitesse.shp\n",
      "circul_giratoire_p.map vitesse.shp\n",
      "circul_giratoire_p.tab vitesse.shp\n",
      "signalisation_lumineuse.dat vitesse.shp\n",
      "signalisation_lumineuse.id vitesse.shp\n",
      "signalisation_lumineuse.ind vitesse.shp\n",
      "signalisation_lumineuse.map vitesse.shp\n",
      "signalisation_lumineuse.tab vitesse.shp\n",
      "tronfilrevetement.dat vitesse.shp\n",
      "tronfilrevetement.id vitesse.shp\n",
      "tronfilrevetement.map vitesse.shp\n",
      "tronfilrevetement.tab vitesse.shp\n",
      "vitesses_agglo.dat vitesse.shp\n",
      "vitesses_agglo.id vitesse.shp\n",
      "vitesses_agglo.ind vitesse.shp\n",
      "vitesses_agglo.map vitesse.shp\n",
      "vitesses_agglo.tab vitesse.shp\n"
     ]
    }
   ],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.Agglo)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.Agglo)) : \n",
    "        if t.Agglo  not in  ('Nantes', 'Quimper') : \n",
    "                continue\n",
    "        for f in files : \n",
    "            print(f.lower(), t.nom_fichier_vitesse.lower())\n",
    "            if f.lower() == t.nom_fichier_vitesse.lower() : \n",
    "                print(f)\n",
    "                #ouvrir et modifier le crs si besoin\n",
    "                dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                dicoNomAttr={t.nom_attr_routevitesse:'nomRoute',t.nom_attr_vtsvl:'vtsvl', t.nom_attr_vtspl:'vtspl'}\n",
    "                listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                \"\"\"if t.Agglo in ('Nantes'):\n",
    "                    print('agglo')\n",
    "                    dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull, typegest='agglo')\n",
    "                else :\"\"\"\n",
    "                dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.Agglo]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corriger les vitesse\n",
    "def modifVitesse(vts):\n",
    "    if pd.isnull(vts) or vts==0 or vts in ('P', 'PV') : \n",
    "        return None\n",
    "    elif vts in ('Z30', 'z30') : \n",
    "        return 30\n",
    "    elif vts in ('ZR20',) : \n",
    "        return 20\n",
    "    elif isinstance(vts, str) : \n",
    "        return int(vts)\n",
    "    elif vts < 20 : \n",
    "        return 30\n",
    "    elif vts in (63, 67) :\n",
    "        return 70\n",
    "    elif vts in(92,): \n",
    "        return 90\n",
    "    elif vts in (58,45) : \n",
    "        return 50\n",
    "    else :\n",
    "        return vts\n",
    "#ajout attribut vts pl pour homogénéïté\n",
    "dfConcatFixe['vtspl']=None\n",
    "dfConcatFixe['vtsvl']=dfConcatFixe.vtsvl.apply(lambda x : modifVitesse(x))\n",
    "dfConcatFixe['vtspl']=dfConcatFixe.vtspl.apply(lambda x : modifVitesse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfConcatFixe.vtsvl.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcat.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\0_ConcatAGglos\\Concat_Agglo_1Attributs_1Fichier_vitesse_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.3 fichier de revetement pour 1 fichier lineaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossierDonneesRd=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\old'\n",
    "fichierSynthese=os.path.join(dossierDonneesRd,'Synthese_type_fichier_agglo.ods')\n",
    "dfFichierSynthese=pd.read_excel(fichierSynthese, sheet_name='Complet',engine='odf', nrows=35, dtype={'dept':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNettoyeesSansReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']=='non') & (dfFichierSynthese['fichier_unique']=='non')].copy()\n",
    "dfNettoyeesAvecReglesGestion=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne')\n",
    "                                 & (dfFichierSynthese['regles_gestion_trafic']!='non') & (dfFichierSynthese['fichier_unique']=='non')].copy()\n",
    "dfNettoyees=dfFichierSynthese.loc[(dfFichierSynthese['valide_trafic']=='oui') & (dfFichierSynthese['type_trafic']=='sig') & (dfFichierSynthese['type_geom_trafic']=='ligne') \n",
    "                                 & (dfFichierSynthese['fichier_unique']=='non')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agglo</th>\n",
       "      <th>nom_fichier_trafic</th>\n",
       "      <th>valide_trafic</th>\n",
       "      <th>type_trafic</th>\n",
       "      <th>type_geom_trafic</th>\n",
       "      <th>nom_attr_trafic</th>\n",
       "      <th>nom_attr_pcpl</th>\n",
       "      <th>nom_attr_route</th>\n",
       "      <th>nom_attr_annee</th>\n",
       "      <th>projection</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_fichier_revetement</th>\n",
       "      <th>valide_revetement</th>\n",
       "      <th>nom_attr_route_rvt</th>\n",
       "      <th>type_revetement</th>\n",
       "      <th>type_geom_revetement</th>\n",
       "      <th>nom_attr_revetement</th>\n",
       "      <th>nom_attr_granulo</th>\n",
       "      <th>nom_attr_annee_pose</th>\n",
       "      <th>regles gestion_revetement</th>\n",
       "      <th>commentaires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quimper</td>\n",
       "      <td>Cerema_Quimper_trafic_ligne.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOM</td>\n",
       "      <td>ANNEE_CPTG</td>\n",
       "      <td>Epsg:2154</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nantes</td>\n",
       "      <td>TraficGeoNantes2018.shp</td>\n",
       "      <td>oui</td>\n",
       "      <td>sig</td>\n",
       "      <td>ligne</td>\n",
       "      <td>Trafic_jou</td>\n",
       "      <td>Trafic_j_1,Trafic_jou</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Epsg:3947</td>\n",
       "      <td>...</td>\n",
       "      <td>TronfilRevetement.shp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIBELLE</td>\n",
       "      <td>REVETEMENT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Agglo               nom_fichier_trafic valide_trafic type_trafic  \\\n",
       "3  Quimper  Cerema_Quimper_trafic_ligne.shp           oui         sig   \n",
       "7   Nantes          TraficGeoNantes2018.shp           oui         sig   \n",
       "\n",
       "  type_geom_trafic nom_attr_trafic          nom_attr_pcpl nom_attr_route  \\\n",
       "3            ligne              TV                    NaN            NOM   \n",
       "7            ligne      Trafic_jou  Trafic_j_1,Trafic_jou            NaN   \n",
       "\n",
       "  nom_attr_annee projection  ... nom_fichier_revetement valide_revetement  \\\n",
       "3     ANNEE_CPTG  Epsg:2154  ...                    NaN               NaN   \n",
       "7            NaN  Epsg:3947  ...  TronfilRevetement.shp               NaN   \n",
       "\n",
       "  nom_attr_route_rvt type_revetement type_geom_revetement  \\\n",
       "3                NaN             NaN                  NaN   \n",
       "7            LIBELLE      REVETEMENT                  NaN   \n",
       "\n",
       "   nom_attr_revetement nom_attr_granulo nom_attr_annee_pose  \\\n",
       "3                  NaN              NaN                 NaN   \n",
       "7                  NaN              NaN                 NaN   \n",
       "\n",
       "  regles gestion_revetement commentaires  \n",
       "3                       NaN          NaN  \n",
       "7                       NaN          NaN  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNettoyees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quimper\n",
      "Nantes\n",
      "['nomRoute', 'rvtType', 'geometry'] {'LIBELLE': 'nomRoute', 'REVETEMENT': 'rvtType', nan: 'granulo', nan: 'anneePose'}\n",
      "agglo\n"
     ]
    }
   ],
   "source": [
    "dicoGdf={}\n",
    "for t in dfNettoyees.itertuples() : \n",
    "    #ouvrir le fichier\n",
    "    print(t.Agglo)\n",
    "    for root, dirs, files in os.walk(os.path.join(dossierDonneesRd, t.Agglo)) : \n",
    "        if t.Agglo  not in  ('Nantes') : \n",
    "                continue\n",
    "        for f in files : \n",
    "            if isinstance(t.nom_fichier_revetement, str) : \n",
    "                #print(f.lower(), t.nom_fichier_vitesse.lower())\n",
    "                if f.lower() == t.nom_fichier_revetement.lower() : \n",
    "                    #print(f)\n",
    "                    #ouvrir et modifier le crs si besoin\n",
    "                    dfFichierSource=ouvrirReprojeter(os.path.join(root, f),t)\n",
    "                    dicoNomAttr={t.nom_attr_route_rvt:'nomRoute',t.type_revetement:'rvtType',t.nom_attr_granulo:'granulo', t.nom_attr_annee_pose : 'anneePose'}\n",
    "                    listAttrNonNull=[v for k,v in dicoNomAttr.items() if not pd.isnull(k)]+['geometry']\n",
    "                    print(listAttrNonNull,dicoNomAttr )\n",
    "                    #a reprendre apres pour le cas des departements particuliers non traites\n",
    "                    if t.Agglo in ('Nantes'):\n",
    "                        print('agglo')\n",
    "                        dfFichierRenomme=calculSpecifique(dfFichierSource, t, listAttrNonNull, typegest='agglo', typeAtr='rvt')\n",
    "                    else :\n",
    "                        dfFichierRenomme=dfFichierSource.rename(columns=dicoNomAttr)[listAttrNonNull]\n",
    "                    ajouterSourceDonnees(t, dfFichierRenomme, f)\n",
    "    dicoGdf[t.Agglo]=dfFichierRenomme\n",
    "dfConcatFixe=pd.concat(dicoGdf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "dfConcatFixe.to_file(r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Reprise PlaMADE-Projet Sword\\Données\\5-Autres gestionnaires\\0_ConcatAGglos\\Concat_Agglo_1Attributs_1Fichier_revetement_lineaire.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer la BdTopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copier la BdTopo en local\n",
    "dossierBox=r'C:\\Users\\martin.schoreisz\\Box\\Projet PLaMADE\\PLAMADE\\Données IGN\\BDTopo\\BD Topo 062019\\3D'\n",
    "dossierLocal=r'D:\\Boulot\\AffairesEnCours\\plamade\\BdTopo2019_06_20'\n",
    "with os.scandir(dossierBox) as it:\n",
    "    for entry in it : \n",
    "        if 'LAMB93' in entry.name and entry.is_file():\n",
    "             shutil.copyfile(os.path.join(dossierBox,entry.name), os.path.join(dossierLocal,entry.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-196ffafd96ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#extraire les fichiers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfichier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D001_2019-06-20.7z.001'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mzip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdossierLocal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfichier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python38\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                 \u001b[1;31m# set the modified flag so central directory gets written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python38\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "#extraire les fichiers\n",
    "fichier='BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D001_2019-06-20.7z.001'\n",
    "zip=zipfile.ZipFile(os.path.join(dossierLocal,fichier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D001_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D002_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D003_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D004_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D005_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D006_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D007_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D008_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D009_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D010_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D011_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D012_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D013_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D014_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D015_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D016_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D017_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D018_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D019_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D021_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D022_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D023_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D024_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D025_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D026_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D027_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D028_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D029_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D02A_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D02B_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D030_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D031_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D032_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D033_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D034_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D035_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D036_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D037_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D038_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D039_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D040_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D041_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D042_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D043_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D044_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D045_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D046_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D047_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D048_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D049_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D050_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D051_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D052_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D053_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D054_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D055_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D056_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D057_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D058_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D059_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D060_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D061_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D062_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D063_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D064_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D065_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D066_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D067_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D068_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D069_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D070_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D071_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D072_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D073_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D074_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D075_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D076_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D077_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D078_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D079_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D080_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D081_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D082_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D083_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D084_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D085_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D086_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D087_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D088_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D089_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D090_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D091_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D092_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D093_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D094_2019-06-20.7z.001',\n",
       " 'BDTOPO_3-0_TOUSTHEMES_SHP_LAMB93_D095_2019-06-20.7z.001']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listeZip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
